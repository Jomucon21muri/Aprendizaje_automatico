{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd185a4c",
   "metadata": {},
   "source": [
    "# ðŸŽ® Reinforcement Learning\n",
    "## Aprendizaje por Refuerzo y Agentes AutÃ³nomos\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/Jomucon21muri/Aprendizaje_automatico/blob/main/01_Sistemas_aprendizaje_automatico/07_Reinforcement_learning/reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Resumen\n",
    "\n",
    "RL permite entrenar agentes que aprenden mediante prueba y error, maximizando recompensas acumuladas.\n",
    "\n",
    "### ðŸŽ¯ Objetivos\n",
    "- **MDPs**: Markov Decision Processes (S, A, P, R, Î³)\n",
    "- **Value Functions**: V(s), Q(s,a)\n",
    "- **Q-Learning**: Algoritmo off-policy tabular\n",
    "- **Deep Q-Networks (DQN)**: Q-learning con redes neuronales\n",
    "- **Policy Gradient**: REINFORCE, PPO, A3C\n",
    "- **Actor-Critic**: Combinar value y policy methods\n",
    "\n",
    "### ðŸ§® FÃ³rmulas Clave\n",
    "\n",
    "**EcuaciÃ³n de Bellman**:\n",
    "$$V(s) = \\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a)V(s') \\right]$$\n",
    "\n",
    "**Q-Learning Update**:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "### ðŸš€ Aplicaciones\n",
    "- **Juegos**: AlphaGo, Dota 2, Chess\n",
    "- **RobÃ³tica**: ManipulaciÃ³n, locomociÃ³n\n",
    "- **Control**: Autopilot, HVAC optimization\n",
    "- **Finanzas**: Trading, portfolio management\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b502636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup y ejemplo Q-Learning\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, states, actions, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.Q = np.zeros((states, actions))\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.Q.shape[1])\n",
    "        return np.argmax(self.Q[state])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        best_next = np.max(self.Q[next_state])\n",
    "        td_target = reward + self.gamma * best_next\n",
    "        td_error = td_target - self.Q[state, action]\n",
    "        self.Q[state, action] += self.alpha * td_error\n",
    "\n",
    "print(\"âœ… Q-Learning Agent implementado\")\n",
    "print(\"\\nðŸ“š Algoritmos RL:\")\n",
    "print(\"  â€¢ Q-Learning: Off-policy, tabular\")\n",
    "print(\"  â€¢ DQN: Q-learning con neural networks\")\n",
    "print(\"  â€¢ PPO: Proximal Policy Optimization\")\n",
    "print(\"  â€¢ A3C: Asynchronous Actor-Critic\")\n",
    "print(\"\\nðŸŽ¯ Componentes Clave:\")\n",
    "print(\"  â€¢ Agent: Toma decisiones\")\n",
    "print(\"  â€¢ Environment: Provee estados y recompensas\")\n",
    "print(\"  â€¢ Policy Ï€(a|s): Estrategia del agente\")\n",
    "print(\"  â€¢ Value Function: PredicciÃ³n de recompensas futuras\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
