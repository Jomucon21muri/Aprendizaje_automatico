{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca2ad0e",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Jomucon21muri/Aprendizaje_automatico/blob/main/01_Sistemas_aprendizaje_automatico/01_Ml_supervisado/ml_supervisado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# üìä Aprendizaje Autom√°tico Supervisado: Fundamentos y Aplicaciones\n",
    "\n",
    "---\n",
    "\n",
    "## Resumen\n",
    "\n",
    "El **aprendizaje supervisado** constituye el paradigma fundamental del aprendizaje autom√°tico moderno, caracterizado por la utilizaci√≥n de conjuntos de datos **etiquetados** para entrenar modelos predictivos.\n",
    "\n",
    "En este notebook exploraremos:\n",
    "\n",
    "- üéØ **Fundamentos te√≥ricos** del aprendizaje supervisado\n",
    "- üìà **Algoritmos de clasificaci√≥n**: √Årboles de decisi√≥n, SVM, Random Forest, etc.\n",
    "- üìâ **Algoritmos de regresi√≥n**: Regresi√≥n lineal, Ridge, Lasso\n",
    "- üî¨ **Ejemplos pr√°cticos** con datasets reales\n",
    "- üìä **Evaluaci√≥n de modelos**: M√©tricas y validaci√≥n\n",
    "- üí° **Casos de uso** en industria\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a655b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n del entorno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.datasets import make_classification, load_iris, load_wine\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "%matplotlib inline\n",
    "\n",
    "print('‚úÖ Entorno configurado correctamente')\n",
    "print(f'NumPy version: {np.__version__}')\n",
    "print(f'Pandas version: {pd.__version__}')\n",
    "print(f'Scikit-learn instalado correctamente')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b5ec9d",
   "metadata": {},
   "source": [
    "## 1. Fundamentos del Aprendizaje Supervisado\n",
    "\n",
    "### 1.1 Definici√≥n Formal\n",
    "\n",
    "El aprendizaje supervisado se define como el problema de aproximar una funci√≥n de mapeo $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$ que relaciona un espacio de entrada $\\mathcal{X}$ con un espacio de salida $\\mathcal{Y}$.\n",
    "\n",
    "**Objetivo**: Minimizar el riesgo esperado:\n",
    "\n",
    "$$R(h) = \\mathbb{E}_{(x,y) \\sim P(\\mathcal{X}, \\mathcal{Y})}[\\mathcal{L}(h(x), y)]$$\n",
    "\n",
    "donde:\n",
    "- $h$ es nuestra hip√≥tesis (modelo)\n",
    "- $\\mathcal{L}$ es la funci√≥n de p√©rdida\n",
    "- $P(\\mathcal{X}, \\mathcal{Y})$ es la distribuci√≥n conjunta de los datos\n",
    "\n",
    "### 1.2 Taxonom√≠a de Problemas\n",
    "\n",
    "**üéØ Clasificaci√≥n**: Espacio de salida discreto\n",
    "- **Binaria**: 2 clases (Spam/No Spam, Fraude/No Fraude)\n",
    "- **Multiclase**: K > 2 clases (Clasificaci√≥n de flores, d√≠gitos)\n",
    "- **Multilabel**: M√∫ltiples etiquetas por instancia\n",
    "\n",
    "**üìà Regresi√≥n**: Espacio de salida continuo\n",
    "- Predicci√≥n de precios\n",
    "- Estimaci√≥n de temperatura\n",
    "- Forecasting de ventas\n",
    "\n",
    "### 1.3 Flujo de Trabajo T√≠pico\n",
    "\n",
    "1. **Recolecci√≥n de datos** etiquetados\n",
    "2. **Preprocesamiento**: Limpieza, normalizaci√≥n, encoding\n",
    "3. **Divisi√≥n**: Train/Validation/Test (60-20-20 o 70-15-15)\n",
    "4. **Entrenamiento**: Ajustar par√°metros del modelo\n",
    "5. **Validaci√≥n**: Ajustar hiperpar√°metros\n",
    "6. **Evaluaci√≥n**: Medir performance en test set\n",
    "7. **Despliegue**: Poner en producci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3bda68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Pr√°ctico: Creaci√≥n de Dataset Sint√©tico para Clasificaci√≥n\n",
    "\n",
    "print(\"üé≤ Generando dataset sint√©tico de clasificaci√≥n binaria...\")\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,           # 1000 ejemplos\n",
    "    n_features=2,             # 2 caracter√≠sticas (para visualizar)\n",
    "    n_informative=2,          # Ambas caracter√≠sticas son informativas\n",
    "    n_redundant=0,            # Sin caracter√≠sticas redundantes\n",
    "    n_clusters_per_class=1,   # 1 cluster por clase\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Crear DataFrame para mejor visualizaci√≥n\n",
    "df = pd.DataFrame(X, columns=['Feature_1', 'Feature_2'])\n",
    "df['Target'] = y\n",
    "\n",
    "print(f\"‚úÖ Dataset generado: {df.shape[0]} ejemplos, {df.shape[1]-1} caracter√≠sticas\")\n",
    "print(f\"\\nDistribuci√≥n de clases:\")\n",
    "print(df['Target'].value_counts())\n",
    "\n",
    "# Visualizaci√≥n del dataset\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Scatter plot de las dos caracter√≠sticas\n",
    "scatter = axes[0].scatter(df['Feature_1'], df['Feature_2'], \n",
    "                         c=df['Target'], cmap='coolwarm', \n",
    "                         alpha=0.6, edgecolors='k', s=50)\n",
    "axes[0].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[0].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[0].set_title('Dataset de Clasificaci√≥n Binaria', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(*scatter.legend_elements(), title=\"Clase\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribuci√≥n de cada caracter√≠stica por clase\n",
    "for feature in ['Feature_1', 'Feature_2']:\n",
    "    for clase in [0, 1]:\n",
    "        data = df[df['Target'] == clase][feature]\n",
    "        axes[1].hist(data, bins=30, alpha=0.5, label=f'Clase {clase} - {feature}')\n",
    "\n",
    "axes[1].set_xlabel('Valor', fontsize=12)\n",
    "axes[1].set_ylabel('Frecuencia', fontsize=12)\n",
    "axes[1].set_title('Distribuciones de Caracter√≠sticas', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Divisi√≥n Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Divisi√≥n de datos:\")\n",
    "print(f\"   Training set: {X_train.shape[0]} ejemplos ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   Test set: {X_test.shape[0]} ejemplos ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\n‚úÖ Datos preparados para entrenamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfad91e",
   "metadata": {},
   "source": [
    "## 2. Algoritmos de Clasificaci√≥n\n",
    "\n",
    "### 2.1 √Årboles de Decisi√≥n\n",
    "\n",
    "Los **√°rboles de decisi√≥n** son modelos jer√°rquicos que particionan el espacio de caracter√≠sticas mediante reglas if-then.\n",
    "\n",
    "**Fundamento Matem√°tico**:\n",
    "\n",
    "**Entrop√≠a de Shannon**:\n",
    "$$H(S) = -\\sum_{c \\in C} p_c \\log_2(p_c)$$\n",
    "\n",
    "**√çndice de Gini**:\n",
    "$$\\text{Gini}(S) = 1 - \\sum_{c \\in C} p_c^2$$\n",
    "\n",
    "**Ventajas**:\n",
    "- ‚úÖ Altamente interpretables\n",
    "- ‚úÖ No requieren normalizaci√≥n\n",
    "- ‚úÖ Manejan caracter√≠sticas num√©ricas y categ√≥ricas\n",
    "- ‚úÖ Capturan relaciones no lineales\n",
    "\n",
    "**Desventajas**:\n",
    "- ‚ùå Alta varianza (inestables)\n",
    "- ‚ùå Tendencia al overfitting\n",
    "- ‚ùå Sesgo hacia caracter√≠sticas con muchos valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e571f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Pr√°ctico: √Årbol de Decisi√≥n\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "print(\"üå≥ Entrenando √Årbol de Decisi√≥n...\")\n",
    "\n",
    "# Crear y entrenar modelo\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    max_depth=4,              # Limitar profundidad para evitar overfitting\n",
    "    min_samples_split=20,     # M√≠nimo de muestras para dividir nodo\n",
    "    min_samples_leaf=10,      # M√≠nimo de muestras en hojas\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_train = dt_model.predict(X_train)\n",
    "y_pred_test = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\nüìä Resultados del √Årbol de Decisi√≥n:\")\n",
    "print(f\"   Training Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"   Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"   Diferencia (overfitting): {(train_acc - test_acc)*100:.2f}%\")\n",
    "\n",
    "# Visualizaci√≥n del √°rbol\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Visualizar estructura del √°rbol\n",
    "plot_tree(dt_model, \n",
    "          feature_names=['Feature_1', 'Feature_2'],\n",
    "          class_names=['Class_0', 'Class_1'],\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          ax=axes[0],\n",
    "          fontsize=10)\n",
    "axes[0].set_title('Estructura del √Årbol de Decisi√≥n', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Frontera de decisi√≥n\n",
    "h = 0.02  # Step size en la malla\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = dt_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "axes[1].contourf(xx, yy, Z, alpha=0.4, cmap='coolwarm')\n",
    "axes[1].scatter(X_test[:, 0], X_test[:, 1], c=y_test, \n",
    "               cmap='coolwarm', edgecolors='k', s=50, alpha=0.8)\n",
    "axes[1].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[1].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[1].set_title('Frontera de Decisi√≥n (Test Set)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Importancia de caracter√≠sticas\n",
    "importances = dt_model.feature_importances_\n",
    "print(f\"\\nüîç Importancia de caracter√≠sticas:\")\n",
    "for i, imp in enumerate(importances):\n",
    "    print(f\"   Feature_{i+1}: {imp:.4f} ({imp*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f02c4",
   "metadata": {},
   "source": [
    "### 2.2 Support Vector Machines (SVM)\n",
    "\n",
    "Las **SVM** buscan el hiperplano √≥ptimo que maximiza el margen entre clases.\n",
    "\n",
    "**Formulaci√≥n Matem√°tica**:\n",
    "\n",
    "Para clasificaci√≥n binaria linealmente separable:\n",
    "\n",
    "$$\\min_{w,b} \\frac{1}{2}\\|w\\|^2$$\n",
    "$$\\text{sujeto a: } y_i(w^T x_i + b) \\geq 1, \\forall i$$\n",
    "\n",
    "**Kernel Trick**: Para problemas no lineales, se mapean datos a espacios de mayor dimensi√≥n mediante kernels:\n",
    "\n",
    "- **Lineal**: $K(x_i, x_j) = x_i^T x_j$\n",
    "- **RBF (Gaussian)**: $K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2)$\n",
    "- **Polinomial**: $K(x_i, x_j) = (\\gamma x_i^T x_j + r)^d$\n",
    "\n",
    "**Ventajas**:\n",
    "- ‚úÖ Efectivo en espacios de alta dimensi√≥n\n",
    "- ‚úÖ Robusto a overfitting (especialmente con regularizaci√≥n)\n",
    "- ‚úÖ Versatilidad mediante kernels\n",
    "\n",
    "**Desventajas**:\n",
    "- ‚ùå Entrenamiento computacionalmente costoso (O(n¬≤) a O(n¬≥))\n",
    "- ‚ùå Dif√≠cil interpretabilidad\n",
    "- ‚ùå Sensible a escalado de caracter√≠sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46ab698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Pr√°ctico: Support Vector Machine\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(\"üéØ Entrenando SVM con diferentes kernels...\")\n",
    "\n",
    "# Normalizar datos (CR√çTICO para SVM)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Entrenar modelos con diferentes kernels\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "svm_models = {}\n",
    "results = {}\n",
    "\n",
    "for kernel in kernels:\n",
    "    print(f\"\\n   Entrenando SVM con kernel {kernel}...\")\n",
    "    \n",
    "    if kernel == 'poly':\n",
    "        model = SVC(kernel=kernel, degree=3, C=1.0, random_state=42)\n",
    "    else:\n",
    "        model = SVC(kernel=kernel, C=1.0, random_state=42)\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    svm_models[kernel] = model\n",
    "    \n",
    "    # Evaluaci√≥n\n",
    "    train_score = model.score(X_train_scaled, y_train)\n",
    "    test_score = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "    results[kernel] = {'train': train_score, 'test': test_score}\n",
    "    print(f\"      Train Acc: {train_score:.4f} | Test Acc: {test_score:.4f}\")\n",
    "\n",
    "# Visualizaci√≥n comparativa\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, kernel in enumerate(kernels):\n",
    "    model = svm_models[kernel]\n",
    "    \n",
    "    # Crear malla para frontera de decisi√≥n\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
    "    y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.4, cmap='coolwarm')\n",
    "    axes[idx].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], \n",
    "                     c=y_test, cmap='coolwarm', edgecolors='k', s=50, alpha=0.8)\n",
    "    \n",
    "    # Marcar support vectors\n",
    "    if hasattr(model, 'support_vectors_'):\n",
    "        axes[idx].scatter(model.support_vectors_[:, 0], \n",
    "                         model.support_vectors_[:, 1],\n",
    "                         s=100, linewidth=1.5, facecolors='none', \n",
    "                         edgecolors='black', label='Support Vectors')\n",
    "    \n",
    "    axes[idx].set_xlabel('Feature 1 (scaled)', fontsize=11)\n",
    "    axes[idx].set_ylabel('Feature 2 (scaled)', fontsize=11)\n",
    "    axes[idx].set_title(f'SVM - Kernel {kernel.upper()}\\nTest Acc: {results[kernel][\"test\"]:.3f}', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend(loc='upper right')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mejor modelo\n",
    "best_kernel = max(results, key=lambda k: results[k]['test'])\n",
    "print(f\"\\nüèÜ Mejor kernel: {best_kernel} con Test Accuracy = {results[best_kernel]['test']:.4f}\")\n",
    "print(f\"   N√∫mero de support vectors: {len(svm_models[best_kernel].support_vectors_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941073d",
   "metadata": {},
   "source": [
    "### 2.3 Random Forest y Ensemble Methods\n",
    "\n",
    "**Random Forest** es un m√©todo de ensemble que combina m√∫ltiples √°rboles de decisi√≥n mediante **bagging** (Bootstrap Aggregating) y **feature randomness**.\n",
    "\n",
    "**Proceso**:\n",
    "1. Crear m√∫ltiples muestras bootstrap del dataset\n",
    "2. Para cada muestra, entrenar un √°rbol con subset aleatorio de caracter√≠sticas\n",
    "3. Agregar predicciones mediante votaci√≥n (clasificaci√≥n) o promedio (regresi√≥n)\n",
    "\n",
    "**Ventajas**:\n",
    "- ‚úÖ Reduce overfitting vs √°rboles individuales\n",
    "- ‚úÖ Robusto a outliers\n",
    "- ‚úÖ Mide importancia de caracter√≠sticas\n",
    "- ‚úÖ Maneja datasets grandes eficientemente\n",
    "\n",
    "**Hiperpar√°metros Clave**:\n",
    "- `n_estimators`: N√∫mero de √°rboles\n",
    "- `max_depth`: Profundidad m√°xima de cada √°rbol\n",
    "- `max_features`: N√∫mero de caracter√≠sticas por split\n",
    "- `min_samples_split`: M√≠nimo de muestras para dividir nodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Pr√°ctico: Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"üå≤üå≤üå≤ Entrenando Random Forest...\")\n",
    "\n",
    "# Entrenar modelo\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,        # 100 √°rboles\n",
    "    max_depth=10,            # Profundidad m√°xima\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',     # sqrt(n_features) por split\n",
    "    random_state=42,\n",
    "    n_jobs=-1                # Usar todos los CPU\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "y_pred_train_rf = rf_model.predict(X_train)\n",
    "y_pred_test_rf = rf_model.predict(X_test)\n",
    "y_proba_test_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "train_acc_rf = accuracy_score(y_train, y_pred_train_rf)\n",
    "test_acc_rf = accuracy_score(y_test, y_pred_test_rf)\n",
    "\n",
    "print(f\"\\nüìä Resultados Random Forest:\")\n",
    "print(f\"   Training Accuracy: {train_acc_rf:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_acc_rf:.4f}\")\n",
    "print(f\"   Diferencia: {(train_acc_rf - test_acc_rf)*100:.2f}%\")\n",
    "\n",
    "# Comparar con √°rbol individual y SVM\n",
    "print(f\"\\nüìà Comparaci√≥n de Modelos (Test Accuracy):\")\n",
    "print(f\"   √Årbol de Decisi√≥n: {test_acc:.4f}\")\n",
    "print(f\"   SVM ({best_kernel}):  {results[best_kernel]['test']:.4f}\")\n",
    "print(f\"   Random Forest:     {test_acc_rf:.4f}\")\n",
    "\n",
    "# Visualizaciones\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Frontera de decisi√≥n\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "Z_rf = rf_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_rf = Z_rf.reshape(xx.shape)\n",
    "\n",
    "axes[0, 0].contourf(xx, yy, Z_rf, alpha=0.4, cmap='coolwarm')\n",
    "axes[0, 0].scatter(X_test[:, 0], X_test[:, 1], c=y_test, \n",
    "                  cmap='coolwarm', edgecolors='k', s=50, alpha=0.8)\n",
    "axes[0, 0].set_title(f'Frontera de Decisi√≥n Random Forest\\nTest Acc: {test_acc_rf:.3f}', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Feature 1')\n",
    "axes[0, 0].set_ylabel('Feature 2')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Importancia de caracter√≠sticas\n",
    "importances_rf = rf_model.feature_importances_\n",
    "indices = np.argsort(importances_rf)[::-1]\n",
    "axes[0, 1].bar(range(len(importances_rf)), importances_rf[indices], color='steelblue')\n",
    "axes[0, 1].set_xticks(range(len(importances_rf)))\n",
    "axes[0, 1].set_xticklabels([f'Feature_{i+1}' for i in indices])\n",
    "axes[0, 1].set_title('Importancia de Caracter√≠sticas', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Importancia')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Curva ROC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba_test_rf)\n",
    "roc_auc = roc_auc_score(y_test, y_proba_test_rf)\n",
    "\n",
    "axes[1, 0].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "axes[1, 0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "axes[1, 0].set_xlim([0.0, 1.0])\n",
    "axes[1, 0].set_ylim([0.0, 1.05])\n",
    "axes[1, 0].set_xlabel('False Positive Rate')\n",
    "axes[1, 0].set_ylabel('True Positive Rate')\n",
    "axes[1, 0].set_title('Curva ROC', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].legend(loc=\"lower right\")\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Matriz de confusi√≥n\n",
    "cm = confusion_matrix(y_test, y_pred_test_rf)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1], \n",
    "            cbar_kws={'label': 'N√∫mero de predicciones'})\n",
    "axes[1, 1].set_title('Matriz de Confusi√≥n', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Clase Real')\n",
    "axes[1, 1].set_xlabel('Clase Predicha')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ M√©tricas detalladas:\")\n",
    "print(classification_report(y_test, y_pred_test_rf, target_names=['Clase 0', 'Clase 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed20bb9",
   "metadata": {},
   "source": [
    "## 3. Algoritmos de Regresi√≥n\n",
    "\n",
    "### 3.1 Regresi√≥n Lineal\n",
    "\n",
    "**Definici√≥n**: Modelar relaci√≥n lineal entre variables predictoras y variable objetivo continua.\n",
    "\n",
    "**Formulaci√≥n Matem√°tica**:\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon$$\n",
    "\n",
    "**Funci√≥n de Costo** (Mean Squared Error):\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Soluci√≥n Anal√≠tica** (Least Squares):\n",
    "$$\\hat{\\beta} = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "### 3.2 Regresi√≥n Ridge (L2 Regularization)\n",
    "\n",
    "A√±ade penalizaci√≥n L2 para prevenir overfitting:\n",
    "\n",
    "$$\\text{Loss} = MSE + \\alpha \\sum_{j=1}^{p}\\beta_j^2$$\n",
    "\n",
    "- $\\alpha$: Par√°metro de regularizaci√≥n (mayor Œ± ‚Üí m√°s regularizaci√≥n)\n",
    "\n",
    "### 3.3 Regresi√≥n Lasso (L1 Regularization)\n",
    "\n",
    "A√±ade penalizaci√≥n L1, que puede llevar coeficientes a cero (feature selection):\n",
    "\n",
    "$$\\text{Loss} = MSE + \\alpha \\sum_{j=1}^{p}|\\beta_j|$$\n",
    "\n",
    "**Comparaci√≥n**:\n",
    "- **Ridge**: Reduce magnitud de coeficientes, no los elimina\n",
    "- **Lasso**: Puede eliminar caracter√≠sticas (sparse solutions)\n",
    "- **Elastic Net**: Combinaci√≥n de L1 y L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fdad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Pr√°ctico: Regresi√≥n Lineal vs Ridge vs Lasso\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "print(\"üìà Generando dataset para regresi√≥n...\")\n",
    "\n",
    "# Generar dataset de regresi√≥n\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=500,\n",
    "    n_features=10,\n",
    "    n_informative=5,\n",
    "    noise=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Dividir datos\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalizar (importante para Ridge/Lasso)\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "print(f\"Dataset: {X_reg.shape[0]} muestras, {X_reg.shape[1]} caracter√≠sticas\")\n",
    "\n",
    "# Entrenar modelos\n",
    "models_reg = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge (Œ±=1.0)': Ridge(alpha=1.0),\n",
    "    'Ridge (Œ±=10.0)': Ridge(alpha=10.0),\n",
    "    'Lasso (Œ±=0.1)': Lasso(alpha=0.1),\n",
    "    'Lasso (Œ±=1.0)': Lasso(alpha=1.0)\n",
    "}\n",
    "\n",
    "results_reg = {}\n",
    "\n",
    "print(f\"\\nüî¨ Entrenando y evaluando modelos de regresi√≥n...\\n\")\n",
    "\n",
    "for name, model in models_reg.items():\n",
    "    model.fit(X_train_reg_scaled, y_train_reg)\n",
    "    \n",
    "    y_pred_train = model.predict(X_train_reg_scaled)\n",
    "    y_pred_test = model.predict(X_test_reg_scaled)\n",
    "    \n",
    "    train_r2 = r2_score(y_train_reg, y_pred_train)\n",
    "    test_r2 = r2_score(y_test_reg, y_pred_test)\n",
    "    test_mse = mean_squared_error(y_test_reg, y_pred_test)\n",
    "    test_mae = mean_absolute_error(y_test_reg, y_pred_test)\n",
    "    \n",
    "    # Contar coeficientes no-cero (para Lasso)\n",
    "    non_zero = np.sum(np.abs(model.coef_) > 1e-5)\n",
    "    \n",
    "    results_reg[name] = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'mse': test_mse,\n",
    "        'mae': test_mae,\n",
    "        'non_zero_coef': non_zero,\n",
    "        'coef': model.coef_\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:20s} | R¬≤ Train: {train_r2:.4f} | R¬≤ Test: {test_r2:.4f} | MSE: {test_mse:.2f} | Features: {non_zero}/{len(model.coef_)}\")\n",
    "\n",
    "# Visualizaciones\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Comparaci√≥n de R¬≤ scores\n",
    "models_names = list(results_reg.keys())\n",
    "train_r2_scores = [results_reg[m]['train_r2'] for m in models_names]\n",
    "test_r2_scores = [results_reg[m]['test_r2'] for m in models_names]\n",
    "\n",
    "x_pos = np.arange(len(models_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x_pos - width/2, train_r2_scores, width, label='Train R¬≤', alpha=0.8, color='steelblue')\n",
    "axes[0, 0].bar(x_pos + width/2, test_r2_scores, width, label='Test R¬≤', alpha=0.8, color='coral')\n",
    "axes[0, 0].set_ylabel('R¬≤ Score')\n",
    "axes[0, 0].set_title('Comparaci√≥n de Modelos de Regresi√≥n', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "axes[0, 0].set_xticklabels(models_names, rotation=45, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0, 0].axhline(y=1.0, color='green', linestyle='--', alpha=0.5, label='Perfect Score')\n",
    "\n",
    "# 2. Visualizaci√≥n de coeficientes\n",
    "for idx, name in enumerate(['Linear Regression', 'Ridge (Œ±=10.0)', 'Lasso (Œ±=1.0)']):\n",
    "    coefs = results_reg[name]['coef']\n",
    "    color = ['steelblue', 'orange', 'green'][idx]\n",
    "    axes[0, 1].plot(range(len(coefs)), coefs, marker='o', label=name, alpha=0.7, linewidth=2)\n",
    "\n",
    "axes[0, 1].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "axes[0, 1].set_xlabel('Feature Index')\n",
    "axes[0, 1].set_ylabel('Coefficient Value')\n",
    "axes[0, 1].set_title('Comparaci√≥n de Coeficientes', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Predicciones vs valores reales (mejor modelo)\n",
    "best_model_name = max(results_reg, key=lambda k: results_reg[k]['test_r2'])\n",
    "best_model = models_reg[best_model_name]\n",
    "y_pred_best = best_model.predict(X_test_reg_scaled)\n",
    "\n",
    "axes[1, 0].scatter(y_test_reg, y_pred_best, alpha=0.6, edgecolors='k', s=50)\n",
    "axes[1, 0].plot([y_test_reg.min(), y_test_reg.max()], \n",
    "               [y_test_reg.min(), y_test_reg.max()], \n",
    "               'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1, 0].set_xlabel('Valores Reales')\n",
    "axes[1, 0].set_ylabel('Valores Predichos')\n",
    "axes[1, 0].set_title(f'Predicciones vs Reales - {best_model_name}\\nR¬≤ = {results_reg[best_model_name][\"test_r2\"]:.4f}', \n",
    "                    fontsize=13, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Residuos\n",
    "residuals = y_test_reg - y_pred_best\n",
    "axes[1, 1].scatter(y_pred_best, residuals, alpha=0.6, edgecolors='k', s=50)\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1, 1].set_xlabel('Valores Predichos')\n",
    "axes[1, 1].set_ylabel('Residuos (Real - Predicho)')\n",
    "axes[1, 1].set_title(f'An√°lisis de Residuos - {best_model_name}', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüèÜ Mejor modelo: {best_model_name}\")\n",
    "print(f\"   Test R¬≤: {results_reg[best_model_name]['test_r2']:.4f}\")\n",
    "print(f\"   Test MSE: {results_reg[best_model_name]['mse']:.2f}\")\n",
    "print(f\"   Test MAE: {results_reg[best_model_name]['mae']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b7454",
   "metadata": {},
   "source": [
    "## 4. M√©tricas de Evaluaci√≥n\n",
    "\n",
    "### 4.1 M√©tricas para Clasificaci√≥n\n",
    "\n",
    "**Matriz de Confusi√≥n**:\n",
    "```\n",
    "                Predicted\n",
    "                Neg    Pos\n",
    "Actual Neg      TN     FP\n",
    "       Pos      FN     TP\n",
    "```\n",
    "\n",
    "**M√©tricas Derivadas**:\n",
    "\n",
    "- **Accuracy**: $\\frac{TP + TN}{TP + TN + FP + FN}$ \n",
    "  - √ötil con clases balanceadas\n",
    "\n",
    "- **Precision**: $\\frac{TP}{TP + FP}$\n",
    "  - \"De los predichos positivos, ¬øcu√°ntos son correctos?\"\n",
    "  - Alta precision ‚Üí Pocos falsos positivos\n",
    "\n",
    "- **Recall (Sensitivity)**: $\\frac{TP}{TP + FN}$\n",
    "  - \"De los positivos reales, ¬øcu√°ntos detectamos?\"\n",
    "  - Alto recall ‚Üí Pocos falsos negativos\n",
    "\n",
    "- **F1-Score**: $2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\n",
    "  - Media arm√≥nica, balance entre precision y recall\n",
    "\n",
    "- **ROC-AUC**: √Årea bajo la curva ROC\n",
    "  - Mide capacidad de discriminaci√≥n del modelo\n",
    "  - 1.0 = Perfecto, 0.5 = Random\n",
    "\n",
    "**¬øCu√°ndo usar cada m√©trica?**\n",
    "- **Accuracy**: Clases balanceadas, costo igual de errores\n",
    "- **Precision**: Minimizar falsos positivos (ej: spam detection)\n",
    "- **Recall**: Minimizar falsos negativos (ej: detecci√≥n de enfermedades)\n",
    "- **F1**: Balance entre precision y recall\n",
    "- **ROC-AUC**: Evaluaci√≥n global, independiente del threshold\n",
    "\n",
    "### 4.2 M√©tricas para Regresi√≥n\n",
    "\n",
    "- **MSE (Mean Squared Error)**: $\\frac{1}{n}\\sum(y_i - \\hat{y}_i)^2$\n",
    "  - Penaliza errores grandes cuadr√°ticamente\n",
    "\n",
    "- **RMSE (Root MSE)**: $\\sqrt{MSE}$\n",
    "  - Misma unidad que variable objetivo\n",
    "\n",
    "- **MAE (Mean Absolute Error)**: $\\frac{1}{n}\\sum|y_i - \\hat{y}_i|$\n",
    "  - M√°s robusto a outliers que MSE\n",
    "\n",
    "- **R¬≤ (Coefficient of Determination)**: $1 - \\frac{SS_{res}}{SS_{tot}}$\n",
    "  - Proporci√≥n de varianza explicada\n",
    "  - 1.0 = Ajuste perfecto, 0.0 = Modelo constante\n",
    "\n",
    "- **MAPE (Mean Absolute Percentage Error)**: $\\frac{100}{n}\\sum\\frac{|y_i - \\hat{y}_i|}{|y_i|}$\n",
    "  - Error relativo en porcentaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956c561",
   "metadata": {},
   "source": [
    "## 5. Caso Pr√°ctico: Dataset Real (Iris)\n",
    "\n",
    "Aplicaremos lo aprendido en un dataset real muy conocido: **Iris Dataset**\n",
    "\n",
    "- 150 muestras de flores\n",
    "- 4 caracter√≠sticas: longitud/ancho de s√©palo y p√©talo\n",
    "- 3 clases: Setosa, Versicolor, Virginica\n",
    "\n",
    "Objetivo: Construir un clasificador multiclase √≥ptimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de9b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso Pr√°ctico Completo: Iris Dataset\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "print(\"üå∫ Cargando Iris Dataset...\")\n",
    "\n",
    "# Cargar datos\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# Crear DataFrame para exploraci√≥n\n",
    "df_iris = pd.DataFrame(X_iris, columns=iris.feature_names)\n",
    "df_iris['species'] = [iris.target_names[i] for i in y_iris]\n",
    "\n",
    "print(f\"\\nDataset: {X_iris.shape[0]} muestras, {X_iris.shape[1]} caracter√≠sticas\")\n",
    "print(f\"\\nPrimeras 5 filas:\")\n",
    "print(df_iris.head())\n",
    "\n",
    "print(f\"\\nDistribuci√≥n de clases:\")\n",
    "print(df_iris['species'].value_counts())\n",
    "\n",
    "# Visualizaci√≥n exploratoria\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Pairplot simplificado\n",
    "colors = ['red', 'green', 'blue']\n",
    "for idx, species in enumerate(iris.target_names):\n",
    "    mask = df_iris['species'] == species\n",
    "    axes[0].scatter(df_iris[mask]['sepal length (cm)'], \n",
    "                   df_iris[mask]['sepal width (cm)'],\n",
    "                   c=colors[idx], label=species, alpha=0.7, s=80, edgecolors='k')\n",
    "\n",
    "axes[0].set_xlabel('Sepal Length (cm)', fontsize=11)\n",
    "axes[0].set_ylabel('Sepal Width (cm)', fontsize=11)\n",
    "axes[0].set_title('Iris Dataset - Sepal Dimensions', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Boxplot de caracter√≠sticas\n",
    "df_iris.iloc[:, :4].boxplot(ax=axes[1])\n",
    "axes[1].set_ylabel('cm', fontsize=11)\n",
    "axes[1].set_title('Distribuci√≥n de Caracter√≠sticas', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Divisi√≥n de datos\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.2, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "# Normalizar\n",
    "scaler_iris = StandardScaler()\n",
    "X_train_iris_scaled = scaler_iris.fit_transform(X_train_iris)\n",
    "X_test_iris_scaled = scaler_iris.transform(X_test_iris)\n",
    "\n",
    "# Entrenar m√∫ltiples modelos\n",
    "models_iris = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=200, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', C=1.0, random_state=42),\n",
    "    'KNN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results_iris = {}\n",
    "\n",
    "print(f\"\\nüî¨ Entrenando y evaluando 6 modelos diferentes...\\n\")\n",
    "\n",
    "for name, model in models_iris.items():\n",
    "    # Entrenar\n",
    "    if name in ['SVM (RBF)', 'KNN (k=5)', 'Logistic Regression']:\n",
    "        model.fit(X_train_iris_scaled, y_train_iris)\n",
    "        y_pred = model.predict(X_test_iris_scaled)\n",
    "    else:\n",
    "        model.fit(X_train_iris, y_train_iris)\n",
    "        y_pred = model.predict(X_test_iris)\n",
    "    \n",
    "    # Evaluar\n",
    "    acc = accuracy_score(y_test_iris, y_pred)\n",
    "    precision = precision_score(y_test_iris, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test_iris, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_iris, y_pred, average='weighted')\n",
    "    \n",
    "    results_iris[name] = {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:20s} | Acc: {acc:.4f} | Prec: {precision:.4f} | Rec: {recall:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "# Visualizaci√≥n de resultados\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Comparaci√≥n de m√©tricas\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "x_pos = np.arange(len(models_iris))\n",
    "width = 0.2\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    values = [results_iris[m][metric] for m in models_iris.keys()]\n",
    "    axes[0].bar(x_pos + idx*width, values, width, label=metric.capitalize(), alpha=0.8)\n",
    "\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Comparaci√≥n de Modelos en Iris Dataset', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xticks(x_pos + width * 1.5)\n",
    "axes[0].set_xticklabels(models_iris.keys(), rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].set_ylim([0.85, 1.01])\n",
    "\n",
    "# Matriz de confusi√≥n del mejor modelo\n",
    "best_model_iris = max(results_iris, key=lambda k: results_iris[k]['accuracy'])\n",
    "cm_iris = confusion_matrix(y_test_iris, results_iris[best_model_iris]['predictions'])\n",
    "\n",
    "sns.heatmap(cm_iris, annot=True, fmt='d', cmap='YlGnBu', ax=axes[1],\n",
    "            xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "axes[1].set_title(f'Matriz de Confusi√≥n - {best_model_iris}\\nAccuracy: {results_iris[best_model_iris][\"accuracy\"]:.4f}', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('Clase Real')\n",
    "axes[1].set_xlabel('Clase Predicha')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüèÜ Mejor modelo: {best_model_iris}\")\n",
    "print(f\"   Accuracy: {results_iris[best_model_iris]['accuracy']:.4f}\")\n",
    "print(f\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test_iris, results_iris[best_model_iris]['predictions'], \n",
    "                           target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182e97a5",
   "metadata": {},
   "source": [
    "## 6. Conclusiones y Mejores Pr√°cticas\n",
    "\n",
    "### üìö Resumen de Conceptos Clave\n",
    "\n",
    "1. **Aprendizaje Supervisado**: Paradigma fundamental del ML que utiliza datos etiquetados para entrenar modelos predictivos\n",
    "\n",
    "2. **Algoritmos Principales**:\n",
    "   - **√Årboles de Decisi√≥n**: Interpretables, √∫tiles para problemas de clasificaci√≥n y regresi√≥n\n",
    "   - **SVM**: Potentes para problemas no lineales mediante kernels\n",
    "   - **Random Forest**: Robustos mediante ensambles de √°rboles\n",
    "   - **Regresi√≥n**: M√∫ltiples variantes (Linear, Ridge, Lasso) para problemas de predicci√≥n num√©rica\n",
    "\n",
    "3. **M√©tricas de Evaluaci√≥n**:\n",
    "   - **Clasificaci√≥n**: Accuracy, Precision, Recall, F1-Score, ROC-AUC\n",
    "   - **Regresi√≥n**: MSE, RMSE, MAE, R¬≤, MAPE\n",
    "\n",
    "### ‚úÖ Mejores Pr√°cticas\n",
    "\n",
    "- **Preprocesamiento**: Normalizar/escalar datos, especialmente para SVM y modelos basados en distancia\n",
    "- **Divisi√≥n de Datos**: Usar train/test split (70-30 o 80-20) y validaci√≥n cruzada\n",
    "- **Selecci√≥n de Modelos**: Comparar m√∫ltiples algoritmos antes de seleccionar el mejor\n",
    "- **Hiperpar√°metros**: Optimizar mediante GridSearch o RandomSearch\n",
    "- **Validaci√≥n**: Siempre evaluar en datos no vistos (test set)\n",
    "- **Interpretabilidad vs Performance**: Balance entre explicabilidad y precisi√≥n\n",
    "\n",
    "### üéØ Ejercicios Propuestos\n",
    "\n",
    "1. **Ejercicio 1**: Implementa un clasificador de d√≠gitos manuscritos usando el dataset MNIST con Random Forest\n",
    "2. **Ejercicio 2**: Compara el rendimiento de diferentes kernels de SVM en un problema de clasificaci√≥n binaria\n",
    "3. **Ejercicio 3**: Crea un modelo de regresi√≥n para predecir precios de casas (Boston Housing dataset)\n",
    "4. **Ejercicio 4**: Implementa validaci√≥n cruzada k-fold y compara con train/test split simple\n",
    "5. **Ejercicio 5**: Optimiza hiperpar√°metros de un modelo usando GridSearchCV\n",
    "\n",
    "### üìñ Recursos Adicionales\n",
    "\n",
    "- **Scikit-learn Documentation**: https://scikit-learn.org/stable/supervised_learning.html\n",
    "- **Libro**: \"Introduction to Statistical Learning\" by James et al.\n",
    "- **Curso**: \"Machine Learning\" de Andrew Ng (Coursera)\n",
    "- **Kaggle Competitions**: Pr√°ctica con problemas reales de ML supervisado\n",
    "\n",
    "---\n",
    "\n",
    "**üéì Siguiente Notebook**: [ML No Supervisado](../02_Ml_no_supervisado/ml_no_supervisado.ipynb)\n",
    "\n",
    "En el siguiente m√≥dulo exploraremos t√©cnicas de aprendizaje no supervisado, incluyendo clustering, reducci√≥n de dimensionalidad y detecci√≥n de anomal√≠as."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
