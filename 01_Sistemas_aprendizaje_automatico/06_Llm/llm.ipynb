{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013e8a3b",
   "metadata": {},
   "source": [
    "# ðŸ¤– Large Language Models (LLMs)\n",
    "## De GPT a ChatGPT y Modelos Multimodales\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/Jomucon21muri/Aprendizaje_automatico/blob/main/01_Sistemas_aprendizaje_automatico/06_Llm/llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Resumen\n",
    "\n",
    "Los LLMs han transformado la IA mediante modelos masivos pre-entrenados capaces de realizar mÃºltiples tareas con pocos o ningÃºn ejemplo.\n",
    "\n",
    "### ðŸŽ¯ Objetivos\n",
    "- **Arquitecturas**: GPT (autoregresivo), BERT (bidireccional), T5 (encoder-decoder)\n",
    "- **Pre-training**: Aprender representaciones de lenguaje de corpus masivos\n",
    "- **Fine-tuning**: Adaptar a tareas especÃ­ficas\n",
    "- **Prompt Engineering**: DiseÃ±ar instrucciones efectivas\n",
    "- **In-Context Learning**: Aprender de ejemplos en el prompt\n",
    "- **RLHF**: AlineaciÃ³n con preferencias humanas\n",
    "\n",
    "### ðŸ“Š Modelos Principales\n",
    "\n",
    "| Modelo | ParÃ¡metros | Tipo | OrganizaciÃ³n |\n",
    "|--------|------------|------|--------------|\n",
    "| GPT-4 | ~1T | Autoregresivo | OpenAI |\n",
    "| GPT-3 | 175B | Autoregresivo | OpenAI |\n",
    "| BERT | 340M | Bidirectional | Google |\n",
    "| T5 | 11B | Encoder-Decoder | Google |\n",
    "| LLaMA | 7B-65B | Autoregresivo | Meta |\n",
    "| Mistral | 7B | Autoregresivo | Mistral AI |\n",
    "| Claude | ? | Autoregresivo | Anthropic |\n",
    "\n",
    "### ðŸ”‘ Capacidades Emergentes\n",
    "- **Few-Shot Learning**: Generalizar desde pocos ejemplos\n",
    "- **Chain-of-Thought**: Razonamiento paso a paso\n",
    "- **Code Generation**: ProgramaciÃ³n asistida por IA\n",
    "- **Multimodal**: Procesar texto + imÃ¡genes (GPT-4V, Claude 3)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd300cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"âœ… LLM notebook ready\")\n",
    "print(\"\\nðŸ’¡ Nota: Para usar transformers, instalar con: pip install transformers torch\")\n",
    "print(\"\\nðŸ“š Conceptos:\")\n",
    "print(\"  â€¢ Pre-training: Aprender de corpus masivo sin supervisiÃ³n\")\n",
    "print(\"  â€¢ Fine-tuning: Adaptar a tarea especÃ­fica con datos etiquetados\")\n",
    "print(\"  â€¢ Prompt Engineering: DiseÃ±ar instrucciones efectivas\")\n",
    "print(\"  â€¢ RLHF: Reinforcement Learning from Human Feedback\")\n",
    "print(\"\\nðŸ”¬ Aplicaciones:\")\n",
    "print(\"  â€¢ Chatbots (ChatGPT, Claude)\")\n",
    "print(\"  â€¢ Code Generation (GitHub Copilot)\")\n",
    "print(\"  â€¢ Translation, Summarization\")\n",
    "print(\"  â€¢ Question Answering\")\n",
    "print(\"\\nðŸ“– Recursos:\")\n",
    "print(\"  â€¢ Hugging Face: https://huggingface.co\")\n",
    "print(\"  â€¢ OpenAI API: https://platform.openai.com\")\n",
    "print(\"  â€¢ Paper: 'Language Models are Few-Shot Learners' (GPT-3)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
