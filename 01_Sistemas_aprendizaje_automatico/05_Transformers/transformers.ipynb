{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3619a647",
   "metadata": {},
   "source": [
    "# üîÑ Transformers: Arquitectura que Revolucion√≥ la IA\n",
    "## Attention Is All You Need\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/Jomucon21muri/Aprendizaje_automatico/blob/main/01_Sistemas_aprendizaje_automatico/05_Transformers/transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Resumen\n",
    "\n",
    "Transformers han revolucionado NLP, visi√≥n computacional y ML en general mediante **mecanismos de atenci√≥n** que permiten procesamiento paralelo y captura de dependencias de largo alcance.\n",
    "\n",
    "### üéØ Objetivos\n",
    "\n",
    "1. **Mecanismos de Atenci√≥n**:\n",
    "   - Self-Attention: $\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "   - Multi-Head Attention\n",
    "   - Query, Key, Value matrices\n",
    "\n",
    "2. **Arquitectura Transformer**:\n",
    "   - Encoder-Decoder structure\n",
    "   - Positional Encoding\n",
    "   - Feed-Forward Networks\n",
    "   - Layer Normalization y Skip Connections\n",
    "\n",
    "3. **Modelos Derivados**:\n",
    "   - **BERT**: Bidirectional Encoder (clasificaci√≥n, NER, QA)\n",
    "   - **GPT**: Autoregressive Decoder (generaci√≥n de texto)\n",
    "   - **T5, BART**: Encoder-Decoder (traducci√≥n, resumen)\n",
    "   - **Vision Transformer (ViT)**: Transformers para im√°genes\n",
    "\n",
    "4. **Implementaci√≥n Pr√°ctica**:\n",
    "   - Atenci√≥n desde cero\n",
    "   - Fine-tuning de modelos pre-entrenados\n",
    "   - Transfer learning con Hugging Face\n",
    "\n",
    "### üìä Contenido\n",
    "\n",
    "- Teor√≠a matem√°tica de atenci√≥n\n",
    "- Implementaci√≥n simplificada de Transformer\n",
    "- Fine-tuning con Hugging Face Transformers\n",
    "- Aplicaciones: clasificaci√≥n de texto, NER, generaci√≥n\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ TensorFlow:\", tf.__version__)\n",
    "print(\"‚úÖ Entorno Transformers configurado\\n\")\n",
    "\n",
    "# Implementaci√≥n simplificada de Self-Attention\n",
    "class SelfAttention(layers.Layer):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.sqrt_d = tf.sqrt(tf.cast(d_model, tf.float32))\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.W_q = self.add_weight(shape=(input_shape[-1], self.d_model), initializer='glorot_uniform')\n",
    "        self.W_k = self.add_weight(shape=(input_shape[-1], self.d_model), initializer='glorot_uniform')\n",
    "        self.W_v = self.add_weight(shape=(input_shape[-1], self.d_model), initializer='glorot_uniform')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Q = tf.matmul(inputs, self.W_q)\n",
    "        K = tf.matmul(inputs, self.W_k)\n",
    "        V = tf.matmul(inputs, self.W_v)\n",
    "        \n",
    "        # Atenci√≥n: softmax(QK^T / sqrt(d_k)) V\n",
    "        scores = tf.matmul(Q, K, transpose_b=True) / self.sqrt_d\n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        output = tf.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "\n",
    "print(\"üì¶ Clase SelfAttention implementada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479178c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Visualizar Atenci√≥n en secuencia simple\n",
    "\n",
    "# Crear secuencia de ejemplo\n",
    "sequence = np.random.randn(1, 10, 64)  # (batch, seq_len, features)\n",
    "\n",
    "# Aplicar Self-Attention\n",
    "attention_layer = SelfAttention(d_model=64)\n",
    "output, attention_weights = attention_layer(sequence)\n",
    "\n",
    "print(f\"Input shape: {sequence.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "\n",
    "# Visualizar matriz de atenci√≥n\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Attention matrix\n",
    "im = ax1.imshow(attention_weights[0].numpy(), cmap='viridis', aspect='auto')\n",
    "ax1.set_title('Matriz de Atenci√≥n (10x10)', fontweight='bold', fontsize=12)\n",
    "ax1.set_xlabel('Key Position')\n",
    "ax1.set_ylabel('Query Position')\n",
    "plt.colorbar(im, ax=ax1, label='Attention Weight')\n",
    "\n",
    "# Atenci√≥n promedio por posici√≥n\n",
    "avg_attention = np.mean(attention_weights[0].numpy(), axis=0)\n",
    "ax2.bar(range(10), avg_attention, color='steelblue', alpha=0.8, edgecolor='black')\n",
    "ax2.set_title('Atenci√≥n Promedio por Posici√≥n', fontweight='bold', fontsize=12)\n",
    "ax2.set_xlabel('Position')\n",
    "ax2.set_ylabel('Average Attention')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Interpretaci√≥n:\")\n",
    "print(\"  ‚Ä¢ Cada celda (i,j) muestra cu√°nta atenci√≥n presta posici√≥n i a posici√≥n j\")\n",
    "print(\"  ‚Ä¢ Filas suman 1 (distribuci√≥n de probabilidad)\")\n",
    "print(\"  ‚Ä¢ Patterns diagonales indican atenci√≥n local\")\n",
    "print(\"  ‚Ä¢ Values altos fuera de diagonal = dependencias de largo alcance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e49a935",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "### üìö Conceptos Clave\n",
    "- **Self-Attention**: Permite procesar secuencias en paralelo y capturar dependencias globales\n",
    "- **Multi-Head Attention**: M√∫ltiples representaciones en diferentes subespacios\n",
    "- **Positional Encoding**: Inyecta informaci√≥n de orden en el modelo\n",
    "- **Encoder-Decoder**: Arquitectura para traducci√≥n y generaci√≥n condicionada\n",
    "\n",
    "### üéØ Modelos Principales\n",
    "\n",
    "| Modelo | Tipo | Uso Principal |\n",
    "|--------|------|---------------|\n",
    "| BERT | Encoder | Clasificaci√≥n, NER, QA |\n",
    "| GPT | Decoder | Generaci√≥n de texto |\n",
    "| T5 | Encoder-Decoder | Tareas seq2seq |\n",
    "| ViT | Encoder | Clasificaci√≥n de im√°genes |\n",
    "\n",
    "### ‚úÖ Ventajas vs RNNs\n",
    "- ‚úÖ Paralelizaci√≥n completa\n",
    "- ‚úÖ Dependencias de largo alcance (O(1) vs O(n))\n",
    "- ‚úÖ Interpretabilidad (visualizar atenci√≥n)\n",
    "- ‚úÖ Transfer learning efectivo\n",
    "\n",
    "### üìñ Recursos\n",
    "- Paper: \"Attention Is All You Need\" (Vaswani et al., 2017)\n",
    "- **Hugging Face**: https://huggingface.co/transformers\n",
    "- **The Illustrated Transformer**: https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "---\n",
    "\n",
    "**üéì Siguiente**: [LLM](../06_Llm/llm.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
