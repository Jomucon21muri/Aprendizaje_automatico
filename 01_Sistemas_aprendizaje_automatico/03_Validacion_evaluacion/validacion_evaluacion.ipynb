{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e4bf95b",
   "metadata": {},
   "source": [
    "# ‚úÖ Validaci√≥n y Evaluaci√≥n de Modelos de ML\n",
    "## T√©cnicas Rigurosas para Garantizar Generalizaci√≥n\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/yourusername/ml-course/blob/main/01_Sistemas_aprendizaje_automatico/03_Validacion_evaluacion/validacion_evaluacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Resumen del Notebook\n",
    "\n",
    "Este notebook cubre **metodolog√≠as rigurosas de validaci√≥n y evaluaci√≥n** para garantizar que los modelos de ML generalicen correctamente.\n",
    "\n",
    "### üéØ Objetivos de Aprendizaje\n",
    "\n",
    "1. **Problem√°ticas de Generalizaci√≥n**:\n",
    "   - Overfitting vs Underfitting\n",
    "   - Trade-off Sesgo-Varianza\n",
    "   - Detecci√≥n y mitigaci√≥n\n",
    "\n",
    "2. **Estrategias de Validaci√≥n**:\n",
    "   - Train-Test Split\n",
    "   - K-Fold Cross-Validation\n",
    "   - Stratified K-Fold\n",
    "   - Leave-One-Out (LOO)\n",
    "   - Time Series Split\n",
    "\n",
    "3. **M√©tricas de Evaluaci√≥n**:\n",
    "   - **Clasificaci√≥n**: Accuracy, Precision, Recall, F1, ROC-AUC, PR-AUC\n",
    "   - **Regresi√≥n**: MSE, RMSE, MAE, R¬≤, MAPE\n",
    "   - Matrices de confusi√≥n y curvas ROC\n",
    "\n",
    "4. **Optimizaci√≥n de Hiperpar√°metros**:\n",
    "   - Grid Search\n",
    "   - Random Search\n",
    "   - Bayesian Optimization\n",
    "\n",
    "### üìä Contenido\n",
    "\n",
    "- Teor√≠a con fundamentaci√≥n matem√°tica\n",
    "- Implementaciones pr√°cticas\n",
    "- Visualizaciones de curvas de aprendizaje\n",
    "- Comparaciones de estrategias de validaci√≥n\n",
    "- Pipeline completo de evaluaci√≥n\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f3cfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n del entorno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score, KFold, \n",
    "                                      StratifiedKFold, LeaveOneOut, TimeSeriesSplit,\n",
    "                                      GridSearchCV, RandomizedSearchCV, learning_curve,\n",
    "                                      validation_curve)\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                            confusion_matrix, classification_report, roc_curve, roc_auc_score,\n",
    "                            precision_recall_curve, average_precision_score,\n",
    "                            mean_squared_error, mean_absolute_error, r2_score,\n",
    "                            mean_absolute_percentage_error)\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification, make_regression, load_breast_cancer, load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"‚úÖ Entorno configurado correctamente\")\n",
    "print(f\"NumPy: {np.__version__} | Pandas: {pd.__version__}\")\n",
    "print(f\"Scikit-learn: M√≥dulos de validaci√≥n, m√©tricas y optimizaci√≥n cargados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2125e51a",
   "metadata": {},
   "source": [
    "## 1. Overfitting vs Underfitting\n",
    "\n",
    "### üéØ Problem√°ticas de Generalizaci√≥n\n",
    "\n",
    "#### Overfitting (Sobreajuste)\n",
    "Modelo aprende **ruido y peculiaridades** del conjunto de entrenamiento en lugar de patrones generales.\n",
    "\n",
    "**S√≠ntomas**:\n",
    "- Error entrenamiento << Error validaci√≥n\n",
    "- Modelo muy complejo para los datos\n",
    "- Rendimiento degrada con datos nuevos\n",
    "\n",
    "#### Underfitting (Subajuste)\n",
    "Modelo es **demasiado simple** para capturar patrones subyacentes.\n",
    "\n",
    "**S√≠ntomas**:\n",
    "- Error entrenamiento alto\n",
    "- Error validaci√≥n alto\n",
    "- Modelo no captura relaciones en los datos\n",
    "\n",
    "### Trade-off Sesgo-Varianza\n",
    "\n",
    "$$\\text{Error Total} = \\text{Sesgo}^2 + \\text{Varianza} + \\text{Ruido Irreducible}$$\n",
    "\n",
    "- **Sesgo alto** ‚Üí Underfitting (modelo simple)\n",
    "- **Varianza alta** ‚Üí Overfitting (modelo complejo)\n",
    "- **Objetivo**: Balance √≥ptimo entre ambos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4644d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostraci√≥n Pr√°ctica: Overfitting vs Underfitting con Curvas de Aprendizaje\n",
    "\n",
    "print(\"üéØ Visualizando Overfitting, Underfitting y el Modelo √ìptimo\\n\")\n",
    "\n",
    "# Generar datos con ruido\n",
    "np.random.seed(42)\n",
    "X_demo = np.sort(np.random.rand(100, 1) * 10, axis=0)\n",
    "y_demo = np.sin(X_demo).ravel() + np.random.randn(100) * 0.3\n",
    "\n",
    "# Crear tres modelos con diferentes complejidades\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "models_complexity = {\n",
    "    'Underfitting\\n(max_depth=1)': DecisionTreeRegressor(max_depth=1, random_state=42),\n",
    "    '√ìptimo\\n(max_depth=5)': DecisionTreeRegressor(max_depth=5, random_state=42),\n",
    "    'Overfitting\\n(max_depth=20)': DecisionTreeRegressor(max_depth=20, random_state=42)\n",
    "}\n",
    "\n",
    "# Dividir datos\n",
    "X_train_demo, X_test_demo, y_train_demo, y_test_demo = train_test_split(\n",
    "    X_demo, y_demo, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "X_plot = np.linspace(0, 10, 500).reshape(-1, 1)\n",
    "\n",
    "for idx, (name, model) in enumerate(models_complexity.items()):\n",
    "    # Entrenar\n",
    "    model.fit(X_train_demo, y_train_demo)\n",
    "    y_train_pred = model.predict(X_train_demo)\n",
    "    y_test_pred = model.predict(X_test_demo)\n",
    "    \n",
    "    # Calcular errores\n",
    "    train_score = r2_score(y_train_demo, y_train_pred)\n",
    "    test_score = r2_score(y_test_demo, y_test_pred)\n",
    "    train_mse = mean_squared_error(y_train_demo, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test_demo, y_test_pred)\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].scatter(X_train_demo, y_train_demo, color='blue', s=40, alpha=0.6, label='Train')\n",
    "    axes[idx].scatter(X_test_demo, y_test_demo, color='red', s=40, alpha=0.6, label='Test')\n",
    "    axes[idx].plot(X_plot, model.predict(X_plot), 'g-', linewidth=2, label='Modelo')\n",
    "    \n",
    "    axes[idx].set_title(f'{name}\\nTrain R¬≤: {train_score:.3f} | Test R¬≤: {test_score:.3f}\\n'\n",
    "                       f'Train MSE: {train_mse:.3f} | Test MSE: {test_mse:.3f}',\n",
    "                       fontweight='bold', fontsize=10)\n",
    "    axes[idx].set_xlabel('X')\n",
    "    axes[idx].set_ylabel('y')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä An√°lisis:\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚ö†Ô∏è  UNDERFITTING:\")\n",
    "print(\"   ‚Ä¢ Train score BAJO, Test score BAJO\")\n",
    "print(\"   ‚Ä¢ Modelo demasiado simple, no captura patrones\")\n",
    "print(\"\\n‚úÖ MODELO √ìPTIMO:\")\n",
    "print(\"   ‚Ä¢ Train score ALTO, Test score ALTO y similar\")\n",
    "print(\"   ‚Ä¢ Generaliza bien a datos nuevos\")\n",
    "print(\"\\n‚ùå OVERFITTING:\")\n",
    "print(\"   ‚Ä¢ Train score MUY ALTO, Test score BAJO\")\n",
    "print(\"   ‚Ä¢ Memoriza ruido del training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df95ae19",
   "metadata": {},
   "source": [
    "## 2. Estrategias de Validaci√≥n\n",
    "\n",
    "### üìä M√©todos de Validaci√≥n\n",
    "\n",
    "#### Train-Test Split Simple\n",
    "Divisi√≥n aleatoria: t√≠picamente 70-30% o 80-20%\n",
    "- ‚úÖ R√°pido, simple\n",
    "- ‚ö†Ô∏è Alta varianza, depende de la partici√≥n\n",
    "\n",
    "#### K-Fold Cross-Validation\n",
    "Divide datos en K folds, entrena K veces\n",
    "- ‚úÖ Estimador m√°s robusto\n",
    "- ‚úÖ Usa todos los datos\n",
    "- ‚ö†Ô∏è K veces m√°s costoso\n",
    "\n",
    "#### Stratified K-Fold\n",
    "K-Fold preservando distribuci√≥n de clases\n",
    "- ‚úÖ Esencial para datos desbalanceados\n",
    "- ‚úÖ Reduce varianza en problemas multiclase\n",
    "\n",
    "#### Leave-One-Out (LOO)\n",
    "K-Fold con K=n (cada muestra es un fold)\n",
    "- ‚úÖ Casi imparcial\n",
    "- ‚ö†Ô∏è Muy costoso, alta varianza\n",
    "\n",
    "#### Time Series Split\n",
    "Validaci√≥n cronol√≥gica (no aleatoria)\n",
    "- ‚úÖ Respeta orden temporal\n",
    "- ‚úÖ Previene data leakage futuro‚Üípasado\n",
    "- üéØ Obligatorio para series temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b4288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Pr√°ctico Comprensivo: Pipeline Completo de Validaci√≥n y Evaluaci√≥n\n",
    "\n",
    "print(\"üî¨ Pipeline Completo: Validaci√≥n Cruzada + M√©tricas + Optimizaci√≥n\\n\")\n",
    "\n",
    "# Cargar dataset de clasificaci√≥n binaria\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer = cancer.data\n",
    "y_cancer = cancer.target\n",
    "\n",
    "print(f\"Dataset: Breast Cancer Wisconsin\")\n",
    "print(f\"  ‚Ä¢ Muestras: {len(y_cancer)}\")\n",
    "print(f\"  ‚Ä¢ Features: {X_cancer.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Clases: {len(np.unique(y_cancer))} (Malignant: {sum(y_cancer==0)}, Benign: {sum(y_cancer==1)})\")\n",
    "\n",
    "# Normalizar\n",
    "scaler_cancer = StandardScaler()\n",
    "X_cancer_scaled = scaler_cancer.fit_transform(X_cancer)\n",
    "\n",
    "# 1. COMPARACI√ìN DE ESTRATEGIAS DE VALIDACI√ìN\n",
    "print(\"\\nüìä 1. COMPARACI√ìN DE ESTRATEGIAS DE VALIDACI√ìN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models_val = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', random_state=42, probability=True),\n",
    "    'Logistic Reg': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "validation_strategies = {\n",
    "    'Train-Test (80-20)': None,  # Handled separately\n",
    "    'K-Fold (5)': KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    'K-Fold (10)': KFold(n_splits=10, shuffle=True, random_state=42),\n",
    "    'Stratified K-Fold (5)': StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "}\n",
    "\n",
    "results_validation = {model_name: {} for model_name in models_val.keys()}\n",
    "\n",
    "for model_name, model in models_val.items():\n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_cancer_scaled, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
    "    )\n",
    "    model_clone = sklearn.base.clone(model)\n",
    "    model_clone.fit(X_train, y_train)\n",
    "    results_validation[model_name]['Train-Test (80-20)'] = model_clone.score(X_test, y_test)\n",
    "    \n",
    "    # Cross-Validation strategies\n",
    "    for strategy_name, cv in validation_strategies.items():\n",
    "        if cv is not None:\n",
    "            scores = cross_val_score(model, X_cancer_scaled, y_cancer, cv=cv, scoring='accuracy')\n",
    "            results_validation[model_name][strategy_name] = scores.mean()\n",
    "\n",
    "# Visualizar resultados\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Tabla de resultados\n",
    "df_results = pd.DataFrame(results_validation).T\n",
    "print(f\"\\n{df_results}\\n\")\n",
    "\n",
    "# Plot 1: Comparaci√≥n de estrategias\n",
    "strategies = list(validation_strategies.keys())\n",
    "x_pos = np.arange(len(strategies))\n",
    "width = 0.25\n",
    "\n",
    "for idx, model_name in enumerate(models_val.keys()):\n",
    "    values = [results_validation[model_name][s] for s in strategies]\n",
    "    ax1.bar(x_pos + idx*width, values, width, label=model_name, alpha=0.8)\n",
    "\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Comparaci√≥n de Estrategias de Validaci√≥n', fontweight='bold', fontsize=12)\n",
    "ax1.set_xticks(x_pos + width)\n",
    "ax1.set_xticklabels(strategies, rotation=15, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.set_ylim([0.85, 1.0])\n",
    "\n",
    "# 2. M√âTRICAS DETALLADAS (usando Random Forest con Stratified K-Fold)\n",
    "print(\"\\nüìà 2. AN√ÅLISIS DETALLADO DE M√âTRICAS (Random Forest)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import sklearn.base\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_cancer_scaled, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# M√©tricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f} (De los predichos como positivos, % correctos)\")\n",
    "print(f\"Recall:    {recall:.4f} (De todos los positivos reales, % detectados)\")\n",
    "print(f\"F1-Score:  {f1:.4f} (Media arm√≥nica Precision-Recall)\")\n",
    "print(f\"ROC-AUC:   {roc_auc:.4f} (√Årea bajo curva ROC)\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot 2: Matriz de Confusi√≥n\n",
    "import sklearn.metrics\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2,\n",
    "           xticklabels=['Malignant', 'Benign'], \n",
    "           yticklabels=['Malignant', 'Benign'])\n",
    "ax2.set_title(f'Matriz de Confusi√≥n\\nAccuracy: {accuracy:.4f}', fontweight='bold', fontsize=12)\n",
    "ax2.set_ylabel('True Label')\n",
    "ax2.set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. CURVA ROC\n",
    "print(\"\\nüìâ 3. CURVA ROC\")\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "fig, (ax3, ax4) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax3.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "ax3.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Random Classifier')\n",
    "ax3.fill_between(fpr, tpr, alpha=0.2)\n",
    "ax3.set_xlabel('False Positive Rate')\n",
    "ax3.set_ylabel('True Positive Rate (Recall)')\n",
    "ax3.set_title('Receiver Operating Characteristic (ROC)', fontweight='bold', fontsize=12)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS (Grid Search)\n",
    "print(\"\\nüîß 4. OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Ejecutando Grid Search...\")\n",
    "grid_search.fit(X_cancer_scaled, y_cancer)\n",
    "\n",
    "print(f\"\\nMejores hiperpar√°metros: {grid_search.best_params_}\")\n",
    "print(f\"Mejor ROC-AUC (CV): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Visualizar resultados de Grid Search (subset)\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "top_10 = results_df.nsmallest(10, 'rank_test_score')[['params', 'mean_test_score', 'std_test_score']]\n",
    "\n",
    "# Plot resultados top 10\n",
    "top_10_sorted = top_10.sort_values('mean_test_score', ascending=True)\n",
    "y_pos = np.arange(len(top_10_sorted))\n",
    "\n",
    "ax4.barh(y_pos, top_10_sorted['mean_test_score'], xerr=top_10_sorted['std_test_score'], \n",
    "        alpha=0.8, color='green', edgecolor='black')\n",
    "ax4.set_yticks(y_pos)\n",
    "ax4.set_yticklabels([f\"Config {i+1}\" for i in range(len(top_10_sorted))], fontsize=9)\n",
    "ax4.set_xlabel('ROC-AUC Score')\n",
    "ax4.set_title('Top 10 Configuraciones (Grid Search)', fontweight='bold', fontsize=12)\n",
    "ax4.axvline(x=grid_search.best_score_, color='red', linestyle='--', linewidth=2, label='Best')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PIPELINE COMPLETO\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚úîÔ∏è  Comparaci√≥n de estrategias de validaci√≥n\")\n",
    "print(\"‚úîÔ∏è  Evaluaci√≥n con m√∫ltiples m√©tricas\")\n",
    "print(\"‚úîÔ∏è  Matriz de confusi√≥n y curva ROC\")\n",
    "print(\"‚úîÔ∏è  Optimizaci√≥n de hiperpar√°metros con Grid Search\")\n",
    "print(\"\\nüí° Mejores pr√°cticas aplicadas:\")\n",
    "print(\"   ‚Ä¢ Normalizaci√≥n de datos\")\n",
    "print(\"   ‚Ä¢ Stratified K-Fold para preservar distribuci√≥n de clases\")\n",
    "print(\"   ‚Ä¢ M√∫ltiples m√©tricas para evaluaci√≥n completa\")\n",
    "print(\"   ‚Ä¢ Cross-validation en optimizaci√≥n de hiperpar√°metros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd512182",
   "metadata": {},
   "source": [
    "## 3. Conclusiones\n",
    "\n",
    "### üìö Conceptos Clave\n",
    "1. **Overfitting/Underfitting**: Balance entre complejidad y generalizaci√≥n\n",
    "2. **Validaci√≥n Cruzada**: K-Fold, Stratified, Time Series Split\n",
    "3. **M√©tricas**: Accuracy, Precision, Recall, F1, ROC-AUC, MSE, R¬≤\n",
    "4. **Optimizaci√≥n**: Grid Search, Random Search, Nested CV\n",
    "\n",
    "### ‚úÖ Mejores Pr√°cticas\n",
    "- Usar Stratified K-Fold para datos desbalanceados\n",
    "- Evaluar con m√∫ltiples m√©tricas (no solo accuracy)\n",
    "- Nested CV para evaluaci√≥n imparcial con optimizaci√≥n de hiperpar√°metros\n",
    "- Normalizar datos para modelos basados en distancia\n",
    "- Time Series Split para datos temporales\n",
    "\n",
    "### üéØ Ejercicios\n",
    "1. Implementa Nested CV y compara con CV simple\n",
    "2. Compara Grid Search vs Random Search en un problema complejo\n",
    "3. Analiza trade-off Precision-Recall en clasificaci√≥n desbalanceada\n",
    "4. Crea curvas de aprendizaje para detectar overfitting/underfitting\n",
    "5. Implementa validaci√≥n temporal para series temporales\n",
    "\n",
    "### üìñ Recursos\n",
    "- Scikit-learn: Model Selection https://scikit-learn.org/stable/model_selection.html\n",
    "- \"The Elements of Statistical Learning\" - Hastie et al.\n",
    "- \"Hands-On Machine Learning\" - Aur√©lien G√©ron\n",
    "\n",
    "---\n",
    "\n",
    "**üéì Siguiente**: [Deep Learning](../04_Deep_learning/deep_learning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
