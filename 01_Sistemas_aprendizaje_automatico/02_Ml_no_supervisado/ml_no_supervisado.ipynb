{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a04ea6d7",
   "metadata": {},
   "source": [
    "# üîç Aprendizaje Autom√°tico No Supervisado\n",
    "## Descubrimiento de Patrones Latentes en Datos\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/yourusername/ml-course/blob/main/01_Sistemas_aprendizaje_automatico/02_Ml_no_supervisado/ml_no_supervisado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Resumen del Notebook\n",
    "\n",
    "Este notebook explora el **aprendizaje no supervisado**, un paradigma fundamental del ML que busca descubrir estructuras inherentes en datos sin etiquetas predefinidas.\n",
    "\n",
    "### üéØ Objetivos de Aprendizaje\n",
    "\n",
    "1. **Algoritmos de Clustering**:\n",
    "   - K-Means: Particionamiento iterativo del espacio de datos\n",
    "   - Clustering Jer√°rquico: Construcci√≥n de dendrogramas\n",
    "   - DBSCAN: Clustering basado en densidad\n",
    "   - Gaussian Mixture Models (GMM): Enfoque probabil√≠stico\n",
    "\n",
    "2. **Reducci√≥n de Dimensionalidad**:\n",
    "   - PCA (Principal Component Analysis): An√°lisis de componentes principales\n",
    "   - t-SNE: Visualizaci√≥n no lineal de datos de alta dimensi√≥n\n",
    "\n",
    "3. **Detecci√≥n de Anomal√≠as**:\n",
    "   - Isolation Forest: Detecci√≥n de outliers\n",
    "   - Local Outlier Factor (LOF)\n",
    "\n",
    "4. **Aplicaciones Pr√°cticas**:\n",
    "   - Segmentaci√≥n de clientes\n",
    "   - Compresi√≥n de im√°genes\n",
    "   - Visualizaci√≥n de datos complejos\n",
    "   - Detecci√≥n de fraude\n",
    "\n",
    "### üìä Contenido\n",
    "\n",
    "- Fundamentos te√≥ricos con formulaciones matem√°ticas\n",
    "- Implementaciones pr√°cticas con Scikit-learn\n",
    "- Visualizaciones interactivas\n",
    "- Comparaci√≥n de algoritmos\n",
    "- Casos de uso reales\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55784796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n del entorno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles, load_digits, load_wine\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import cdist\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"‚úÖ Entorno configurado correctamente\")\n",
    "print(f\"NumPy: {np.__version__} | Pandas: {pd.__version__}\")\n",
    "print(f\"Scikit-learn disponible con algoritmos de clustering y reducci√≥n de dimensionalidad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82d6e6",
   "metadata": {},
   "source": [
    "## 1. Fundamentos del Aprendizaje No Supervisado\n",
    "\n",
    "### üéì Marco Conceptual\n",
    "\n",
    "El aprendizaje no supervisado trabaja con conjuntos de datos $\\mathcal{D} = \\{x_i\\}_{i=1}^{n}$ donde $x_i \\in \\mathbb{R}^d$ son observaciones **sin etiquetas** asociadas.\n",
    "\n",
    "#### Objetivos Principales:\n",
    "\n",
    "1. **Descubrimiento de Estructura**: Identificar agrupamientos naturales o patrones de similitud\n",
    "2. **Reducci√≥n de Dimensionalidad**: Encontrar representaciones compactas preservando informaci√≥n relevante\n",
    "3. **Detecci√≥n de Anomal√≠as**: Identificar observaciones que se desv√≠an de patrones normales\n",
    "4. **Aprendizaje de Representaciones**: Descubrir caracter√≠sticas latentes\n",
    "\n",
    "#### Desaf√≠os Metodol√≥gicos:\n",
    "\n",
    "- **Ausencia de Ground Truth**: Sin etiquetas verdaderas, la evaluaci√≥n es fundamentalmente subjetiva\n",
    "- **Definici√≥n de Similitud**: La noci√≥n de similitud puede ser espec√≠fica del dominio\n",
    "- **Escalabilidad**: Muchos algoritmos tienen complejidad computacional prohibitiva\n",
    "- **Interpretabilidad**: Los resultados requieren validaci√≥n mediante conocimiento del dominio\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Clustering: K-Means\n",
    "\n",
    "### üìê Formulaci√≥n Matem√°tica\n",
    "\n",
    "K-Means minimiza la **inercia intra-cluster** (within-cluster sum of squares):\n",
    "\n",
    "$$J = \\sum_{k=1}^{K}\\sum_{x_i \\in C_k}\\|x_i - \\mu_k\\|^2$$\n",
    "\n",
    "donde $\\mu_k$ es el centroide del cluster $C_k$.\n",
    "\n",
    "### Algoritmo de Lloyd:\n",
    "\n",
    "1. **Inicializaci√≥n**: Seleccionar $K$ centroides (K-Means++ recomendado)\n",
    "2. **Asignaci√≥n**: Asignar cada punto al centroide m√°s cercano\n",
    "3. **Actualizaci√≥n**: Recalcular centroides como media de puntos asignados\n",
    "4. **Iteraci√≥n**: Repetir hasta convergencia\n",
    "\n",
    "### ‚úÖ Ventajas:\n",
    "- Eficiencia: $O(nKdi)$ (n=muestras, K=clusters, d=dimensiones, i=iteraciones)\n",
    "- Escalabilidad a grandes datasets\n",
    "- Convergencia garantizada a m√≠nimo local\n",
    "\n",
    "### ‚ö†Ô∏è Limitaciones:\n",
    "- Requiere especificar $K$ a priori\n",
    "- Asume clusters esf√©ricos de tama√±o similar\n",
    "- Sensible a outliers y inicializaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb51d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Pr√°ctico: K-Means en Datasets Sint√©ticos\n",
    "\n",
    "print(\"üî¨ Generando datasets sint√©ticos con diferentes estructuras...\\n\")\n",
    "\n",
    "# Crear tres tipos de datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "# Dataset 1: Clusters esf√©ricos (ideal para K-Means)\n",
    "X_blobs, y_blobs_true = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)\n",
    "\n",
    "# Dataset 2: Clusters en forma de lunas (desafiante para K-Means)\n",
    "X_moons, y_moons_true = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "\n",
    "# Dataset 3: Clusters circulares conc√©ntricos\n",
    "X_circles, y_circles_true = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)\n",
    "\n",
    "datasets = [\n",
    "    (X_blobs, y_blobs_true, \"Clusters Esf√©ricos\"),\n",
    "    (X_moons, y_moons_true, \"Clusters en Lunas\"),\n",
    "    (X_circles, y_circles_true, \"Clusters Circulares\")\n",
    "]\n",
    "\n",
    "# Aplicar K-Means a cada dataset\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 14))\n",
    "\n",
    "for idx, (X, y_true, name) in enumerate(datasets):\n",
    "    # Datos originales\n",
    "    axes[idx, 0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.7, edgecolors='k')\n",
    "    axes[idx, 0].set_title(f'{name}\\n(Ground Truth)', fontweight='bold', fontsize=11)\n",
    "    axes[idx, 0].set_xlabel('Feature 1')\n",
    "    axes[idx, 0].set_ylabel('Feature 2')\n",
    "    \n",
    "    # K-Means clustering\n",
    "    n_clusters = len(np.unique(y_true))\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42, n_init=10)\n",
    "    y_kmeans = kmeans.fit_predict(X)\n",
    "    \n",
    "    axes[idx, 1].scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='plasma', s=50, alpha=0.7, edgecolors='k')\n",
    "    axes[idx, 1].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "                        c='red', marker='X', s=300, edgecolors='black', linewidths=2, \n",
    "                        label='Centroides', zorder=10)\n",
    "    axes[idx, 1].set_title(f'K-Means (K={n_clusters})\\nInercia: {kmeans.inertia_:.2f}', \n",
    "                          fontweight='bold', fontsize=11)\n",
    "    axes[idx, 1].set_xlabel('Feature 1')\n",
    "    axes[idx, 1].legend()\n",
    "    \n",
    "    # Elbow method para determinar K √≥ptimo\n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    K_range = range(2, 11)\n",
    "    \n",
    "    for k in K_range:\n",
    "        km = KMeans(n_clusters=k, init='k-means++', random_state=42, n_init=10)\n",
    "        km.fit(X)\n",
    "        inertias.append(km.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(X, km.labels_))\n",
    "    \n",
    "    ax_twin = axes[idx, 2].twinx()\n",
    "    \n",
    "    axes[idx, 2].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8, label='Inercia')\n",
    "    axes[idx, 2].axvline(x=n_clusters, color='red', linestyle='--', linewidth=2, alpha=0.7, label=f'K √≥ptimo={n_clusters}')\n",
    "    axes[idx, 2].set_xlabel('N√∫mero de Clusters (K)')\n",
    "    axes[idx, 2].set_ylabel('Inercia', color='b')\n",
    "    axes[idx, 2].tick_params(axis='y', labelcolor='b')\n",
    "    axes[idx, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    ax_twin.plot(K_range, silhouette_scores, 'gs-', linewidth=2, markersize=8, label='Silhouette')\n",
    "    ax_twin.set_ylabel('Silhouette Score', color='g')\n",
    "    ax_twin.tick_params(axis='y', labelcolor='g')\n",
    "    \n",
    "    axes[idx, 2].set_title(f'Elbow Method - {name}', fontweight='bold', fontsize=11)\n",
    "    axes[idx, 2].legend(loc='upper right')\n",
    "    ax_twin.legend(loc='center right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä An√°lisis de Resultados:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ Clusters Esf√©ricos: K-Means funciona perfectamente\")\n",
    "print(\"‚ö†Ô∏è  Clusters en Lunas: K-Means falla (geometr√≠a no convexa)\")\n",
    "print(\"‚ö†Ô∏è  Clusters Circulares: K-Means no captura estructura conc√©ntrica\")\n",
    "print(\"\\nüí° Lecci√≥n: K-Means es efectivo solo para clusters esf√©ricos y bien separados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80b2107",
   "metadata": {},
   "source": [
    "## 3. Clustering Jer√°rquico\n",
    "\n",
    "### üå≥ Construcci√≥n de Jerarqu√≠as\n",
    "\n",
    "El clustering jer√°rquico construye una **jerarqu√≠a de agrupamientos** representada mediante un **dendrograma**, permitiendo visualizaci√≥n a m√∫ltiples escalas.\n",
    "\n",
    "#### Enfoque Agglomerative (Bottom-Up):\n",
    "1. Inicializar cada punto como cluster individual\n",
    "2. Iterativamente fusionar los dos clusters m√°s similares\n",
    "3. Continuar hasta obtener un √∫nico cluster\n",
    "\n",
    "#### Criterios de Enlace (Linkage):\n",
    "\n",
    "- **Single Linkage**: $d(C_i, C_j) = \\min_{x \\in C_i, y \\in C_j} d(x,y)$ \n",
    "  - Puede producir efecto \"encadenamiento\"\n",
    "  \n",
    "- **Complete Linkage**: $d(C_i, C_j) = \\max_{x \\in C_i, y \\in C_j} d(x,y)$\n",
    "  - Produce clusters m√°s compactos\n",
    "  \n",
    "- **Average Linkage**: $d(C_i, C_j) = \\frac{1}{|C_i||C_j|}\\sum_{x \\in C_i}\\sum_{y \\in C_j} d(x,y)$\n",
    "  - Balance entre single y complete\n",
    "  \n",
    "- **Ward's Method**: Minimiza incremento en varianza intra-cluster\n",
    "  - M√°s robusto, recomendado en general\n",
    "\n",
    "### ‚úÖ Ventajas:\n",
    "- No requiere especificar $K$ a priori\n",
    "- Dendrograma visualiza estructura jer√°rquica completa\n",
    "- Determin√≠stico (sin inicializaci√≥n aleatoria)\n",
    "\n",
    "### ‚ö†Ô∏è Limitaciones:\n",
    "- Complejidad: $O(n^2\\log n)$ \n",
    "- No escala a datasets masivos\n",
    "- Decisiones de fusi√≥n son irrevocables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Pr√°ctico: Clustering Jer√°rquico con Dendrogramas\n",
    "\n",
    "print(\"üå≥ Aplicando Clustering Jer√°rquico con diferentes m√©todos de enlace...\\n\")\n",
    "\n",
    "# Generar datos con clusters claros\n",
    "np.random.seed(42)\n",
    "X_hier, y_hier = make_blobs(n_samples=150, centers=3, cluster_std=0.8, random_state=42)\n",
    "\n",
    "# M√©todos de enlace a comparar\n",
    "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Crear dendrogramas para cada m√©todo\n",
    "for idx, method in enumerate(linkage_methods):\n",
    "    # Subplot para dendrograma\n",
    "    ax1 = plt.subplot(2, 4, idx + 1)\n",
    "    \n",
    "    # Calcular linkage\n",
    "    Z = linkage(X_hier, method=method)\n",
    "    \n",
    "    # Crear dendrograma\n",
    "    dendrogram(Z, ax=ax1, color_threshold=0.3*max(Z[:, 2]))\n",
    "    ax1.set_title(f'Dendrograma\\n{method.capitalize()} Linkage', fontweight='bold', fontsize=11)\n",
    "    ax1.set_xlabel('√çndice de Muestra')\n",
    "    ax1.set_ylabel('Distancia')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Subplot para clustering resultante\n",
    "    ax2 = plt.subplot(2, 4, idx + 5)\n",
    "    \n",
    "    # Aplicar clustering jer√°rquico\n",
    "    agg = AgglomerativeClustering(n_clusters=3, linkage=method)\n",
    "    y_agg = agg.fit_predict(X_hier)\n",
    "    \n",
    "    # Visualizar\n",
    "    scatter = ax2.scatter(X_hier[:, 0], X_hier[:, 1], c=y_agg, cmap='tab10', \n",
    "                         s=80, alpha=0.7, edgecolors='k', linewidths=1)\n",
    "    ax2.set_title(f'Clusters (K=3)\\n{method.capitalize()} Linkage', fontweight='bold', fontsize=11)\n",
    "    ax2.set_xlabel('Feature 1')\n",
    "    ax2.set_ylabel('Feature 2')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    silhouette = silhouette_score(X_hier, y_agg)\n",
    "    davies_bouldin = davies_bouldin_score(X_hier, y_agg)\n",
    "    calinski = calinski_harabasz_score(X_hier, y_agg)\n",
    "    \n",
    "    # A√±adir texto con m√©tricas\n",
    "    metrics_text = f'Silhouette: {silhouette:.3f}\\nDavies-Bouldin: {davies_bouldin:.3f}'\n",
    "    ax2.text(0.05, 0.95, metrics_text, transform=ax2.transAxes, \n",
    "            fontsize=9, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparaci√≥n detallada de m√©tricas\n",
    "print(\"\\nüìä Comparaci√≥n de M√©todos de Enlace:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'M√©todo':<15} {'Silhouette':<15} {'Davies-Bouldin':<20} {'Calinski-Harabasz'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for method in linkage_methods:\n",
    "    agg = AgglomerativeClustering(n_clusters=3, linkage=method)\n",
    "    y_pred = agg.fit_predict(X_hier)\n",
    "    \n",
    "    sil = silhouette_score(X_hier, y_pred)\n",
    "    db = davies_bouldin_score(X_hier, y_pred)\n",
    "    ch = calinski_harabasz_score(X_hier, y_pred)\n",
    "    \n",
    "    print(f\"{method.capitalize():<15} {sil:<15.4f} {db:<20.4f} {ch:.2f}\")\n",
    "\n",
    "print(\"\\nüí° Interpretaci√≥n de M√©tricas:\")\n",
    "print(\"   ‚Ä¢ Silhouette Score: Mayor es mejor (rango [-1, 1])\")\n",
    "print(\"   ‚Ä¢ Davies-Bouldin: Menor es mejor (mide separaci√≥n entre clusters)\")\n",
    "print(\"   ‚Ä¢ Calinski-Harabasz: Mayor es mejor (ratio varianza inter/intra cluster)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be4181",
   "metadata": {},
   "source": [
    "## 4. DBSCAN (Density-Based Spatial Clustering)\n",
    "\n",
    "### üéØ Clustering Basado en Densidad\n",
    "\n",
    "DBSCAN identifica clusters como **regiones de alta densidad** separadas por regiones de baja densidad, permitiendo descubrir clusters de forma arbitraria.\n",
    "\n",
    "#### Conceptos Fundamentales:\n",
    "\n",
    "- **Œµ-vecindad**: $N_\\epsilon(x) = \\{y \\in \\mathcal{D} : d(x,y) \\leq \\epsilon\\}$\n",
    "- **Core Point**: Punto con al menos `min_samples` vecinos en $N_\\epsilon(x)$\n",
    "- **Border Point**: No es core point pero est√° en vecindad de core point\n",
    "- **Noise Point**: Ni core ni border point (outliers)\n",
    "\n",
    "#### Algoritmo:\n",
    "1. Para cada punto no visitado, determinar si es core point\n",
    "2. Si es core point, iniciar nuevo cluster y a√±adir todos puntos density-reachable\n",
    "3. Puntos no alcanzables se clasifican como **ruido**\n",
    "\n",
    "### ‚úÖ Propiedades Distintivas:\n",
    "- **No requiere especificar K**: N√∫mero de clusters emerge de los datos\n",
    "- **Forma Arbitraria**: No limitado a clusters convexos/esf√©ricos\n",
    "- **Robusto a Ruido**: Identifica y marca outliers expl√≠citamente\n",
    "- **Escalabilidad**: $O(n\\log n)$ con estructuras de indexaci√≥n espacial\n",
    "\n",
    "### ‚ö†Ô∏è Consideraciones:\n",
    "- Requiere selecci√≥n cuidadosa de $\\epsilon$ y `min_samples`\n",
    "- Dificultad con clusters de densidades muy variables\n",
    "- Sensible a escala de caracter√≠sticas (normalizaci√≥n recomendada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a348820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Pr√°ctico: DBSCAN vs K-Means en Datos No Esf√©ricos\n",
    "\n",
    "print(\"üî¨ Comparando DBSCAN con K-Means en datasets complejos...\\n\")\n",
    "\n",
    "# Crear datasets con formas no convexas\n",
    "np.random.seed(42)\n",
    "datasets_complex = [\n",
    "    (make_moons(n_samples=300, noise=0.05, random_state=42)[0], \"Lunas\"),\n",
    "    (make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)[0], \"C√≠rculos\"),\n",
    "    (make_blobs(n_samples=300, centers=[[0,0], [3,3], [0,3]], cluster_std=[0.4, 0.4, 0.4], random_state=42)[0], \"Blobs Irregulares\")\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(len(datasets_complex), 3, figsize=(16, 12))\n",
    "\n",
    "for idx, (X, name) in enumerate(datasets_complex):\n",
    "    # 1. Datos originales\n",
    "    axes[idx, 0].scatter(X[:, 0], X[:, 1], c='gray', s=50, alpha=0.6, edgecolors='k')\n",
    "    axes[idx, 0].set_title(f'{name}\\n(Datos Originales)', fontweight='bold', fontsize=11)\n",
    "    axes[idx, 0].set_xlabel('Feature 1')\n",
    "    axes[idx, 0].set_ylabel('Feature 2')\n",
    "    axes[idx, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. K-Means\n",
    "    kmeans = KMeans(n_clusters=2, init='k-means++', random_state=42, n_init=10)\n",
    "    y_kmeans = kmeans.fit_predict(X)\n",
    "    \n",
    "    axes[idx, 1].scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', s=50, alpha=0.7, edgecolors='k')\n",
    "    axes[idx, 1].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "                        c='red', marker='X', s=300, edgecolors='black', linewidths=2, zorder=10)\n",
    "    silhouette_km = silhouette_score(X, y_kmeans)\n",
    "    axes[idx, 1].set_title(f'K-Means (K=2)\\nSilhouette: {silhouette_km:.3f}', fontweight='bold', fontsize=11)\n",
    "    axes[idx, 1].set_xlabel('Feature 1')\n",
    "    axes[idx, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. DBSCAN\n",
    "    # Normalizar datos para DBSCAN\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Ajustar par√°metros seg√∫n dataset\n",
    "    if name == \"Lunas\":\n",
    "        eps, min_samples = 0.15, 5\n",
    "    elif name == \"C√≠rculos\":\n",
    "        eps, min_samples = 0.15, 5\n",
    "    else:\n",
    "        eps, min_samples = 0.3, 5\n",
    "    \n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    y_dbscan = dbscan.fit_predict(X_scaled)\n",
    "    \n",
    "    # Visualizar resultados (distinguir ruido)\n",
    "    unique_labels = set(y_dbscan)\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Ruido en negro\n",
    "            col = [0, 0, 0, 1]\n",
    "            marker = 'x'\n",
    "            size = 30\n",
    "            label = 'Ruido'\n",
    "        else:\n",
    "            marker = 'o'\n",
    "            size = 50\n",
    "            label = f'Cluster {k}'\n",
    "        \n",
    "        class_mask = (y_dbscan == k)\n",
    "        axes[idx, 2].scatter(X[class_mask, 0], X[class_mask, 1], \n",
    "                           c=[col], marker=marker, s=size, alpha=0.7, \n",
    "                           edgecolors='k', linewidths=1, label=label)\n",
    "    \n",
    "    # Calcular silhouette solo para puntos no-ruido\n",
    "    if len(set(y_dbscan)) > 1 and -1 not in y_dbscan:\n",
    "        silhouette_db = silhouette_score(X, y_dbscan)\n",
    "    else:\n",
    "        mask_no_noise = y_dbscan != -1\n",
    "        if np.sum(mask_no_noise) > 0 and len(set(y_dbscan[mask_no_noise])) > 1:\n",
    "            silhouette_db = silhouette_score(X[mask_no_noise], y_dbscan[mask_no_noise])\n",
    "        else:\n",
    "            silhouette_db = 0.0\n",
    "    \n",
    "    n_clusters = len(set(y_dbscan)) - (1 if -1 in y_dbscan else 0)\n",
    "    n_noise = list(y_dbscan).count(-1)\n",
    "    \n",
    "    axes[idx, 2].set_title(f'DBSCAN (Œµ={eps}, min={min_samples})\\n'\n",
    "                          f'Clusters: {n_clusters} | Ruido: {n_noise} | Sil: {silhouette_db:.3f}',\n",
    "                          fontweight='bold', fontsize=11)\n",
    "    axes[idx, 2].set_xlabel('Feature 1')\n",
    "    axes[idx, 2].grid(True, alpha=0.3)\n",
    "    axes[idx, 2].legend(fontsize=8, loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Conclusiones:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ DBSCAN:\")\n",
    "print(\"   ‚Ä¢ Captura formas no convexas (lunas, c√≠rculos)\")\n",
    "print(\"   ‚Ä¢ Identifica ruido autom√°ticamente\")\n",
    "print(\"   ‚Ä¢ No requiere especificar K\")\n",
    "print(\"\\n‚ö†Ô∏è  K-Means:\")\n",
    "print(\"   ‚Ä¢ Falla con geometr√≠as complejas\")\n",
    "print(\"   ‚Ä¢ Asume clusters esf√©ricos\")\n",
    "print(\"   ‚Ä¢ Todos los puntos asignados (sin detecci√≥n de ruido)\")\n",
    "print(\"\\nüí° Recomendaci√≥n: DBSCAN para datos con formas irregulares y outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eacbfbb",
   "metadata": {},
   "source": [
    "## 5. Gaussian Mixture Models (GMM)\n",
    "\n",
    "### üé≤ Clustering Probabil√≠stico\n",
    "\n",
    "GMM modela la distribuci√≥n de datos como superposici√≥n de $K$ distribuciones gaussianas multivariadas:\n",
    "\n",
    "$$p(x) = \\sum_{k=1}^{K}\\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k)$$\n",
    "\n",
    "donde:\n",
    "- $\\pi_k$: pesos de mezcla ($\\sum_k \\pi_k = 1$)\n",
    "- $\\mu_k$: medias de cada componente\n",
    "- $\\Sigma_k$: matrices de covarianza\n",
    "\n",
    "### Algoritmo Expectation-Maximization (EM)\n",
    "\n",
    "#### E-step (Expectation):\n",
    "Calcular responsabilidades (probabilidades posteriores):\n",
    "\n",
    "$$\\gamma_{ik} = \\frac{\\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j \\mathcal{N}(x_i|\\mu_j, \\Sigma_j)}$$\n",
    "\n",
    "#### M-step (Maximization):\n",
    "Actualizar par√°metros:\n",
    "\n",
    "$$\\mu_k = \\frac{\\sum_i \\gamma_{ik}x_i}{\\sum_i \\gamma_{ik}}, \\quad \\Sigma_k = \\frac{\\sum_i \\gamma_{ik}(x_i - \\mu_k)(x_i - \\mu_k)^T}{\\sum_i \\gamma_{ik}}$$\n",
    "\n",
    "### ‚úÖ Ventajas:\n",
    "- **Soft Clustering**: Pertenencia probabil√≠stica (no binaria)\n",
    "- **Modelo Generativo**: Permite muestrear nuevos puntos\n",
    "- **Flexibilidad**: Clusters el√≠pticos mediante covarianzas\n",
    "- **Fundamento Te√≥rico**: Base estad√≠stica rigurosa\n",
    "\n",
    "### ‚ö†Ô∏è Limitaciones:\n",
    "- Sensible a inicializaci√≥n (similar a K-Means)\n",
    "- Convergencia a m√°ximos locales\n",
    "- Requiere especificar K\n",
    "- Asume forma gaussiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Pr√°ctico: GMM - Soft Clustering y Modelo Generativo\n",
    "\n",
    "print(\"üé≤ Aplicando Gaussian Mixture Models...\\n\")\n",
    "\n",
    "# Generar datos con clusters el√≠pticos\n",
    "np.random.seed(42)\n",
    "from sklearn.datasets import make_blobs\n",
    "X_gmm, y_gmm_true = make_blobs(n_samples=300, centers=3, cluster_std=[0.5, 0.8, 0.6], random_state=42)\n",
    "\n",
    "# Transformaci√≥n para crear clusters el√≠pticos\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_gmm = np.dot(X_gmm, transformation)\n",
    "\n",
    "# Aplicar K-Means y GMM\n",
    "kmeans_gmm = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "y_kmeans_gmm = kmeans_gmm.fit_predict(X_gmm)\n",
    "\n",
    "gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42, n_init=10)\n",
    "gmm.fit(X_gmm)\n",
    "y_gmm_pred = gmm.predict(X_gmm)\n",
    "y_gmm_proba = gmm.predict_proba(X_gmm)\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Ground Truth\n",
    "axes[0, 0].scatter(X_gmm[:, 0], X_gmm[:, 1], c=y_gmm_true, cmap='viridis', s=50, alpha=0.7, edgecolors='k')\n",
    "axes[0, 0].set_title('Ground Truth', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_xlabel('Feature 1')\n",
    "axes[0, 0].set_ylabel('Feature 2')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. K-Means (Hard Clustering)\n",
    "axes[0, 1].scatter(X_gmm[:, 0], X_gmm[:, 1], c=y_kmeans_gmm, cmap='plasma', s=50, alpha=0.7, edgecolors='k')\n",
    "axes[0, 1].scatter(kmeans_gmm.cluster_centers_[:, 0], kmeans_gmm.cluster_centers_[:, 1],\n",
    "                  c='red', marker='X', s=300, edgecolors='black', linewidths=2, zorder=10)\n",
    "silhouette_km_gmm = silhouette_score(X_gmm, y_kmeans_gmm)\n",
    "axes[0, 1].set_title(f'K-Means (Hard Clustering)\\nSilhouette: {silhouette_km_gmm:.3f}', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Feature 1')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. GMM (Hard Clustering)\n",
    "axes[0, 2].scatter(X_gmm[:, 0], X_gmm[:, 1], c=y_gmm_pred, cmap='Set1', s=50, alpha=0.7, edgecolors='k')\n",
    "axes[0, 2].scatter(gmm.means_[:, 0], gmm.means_[:, 1],\n",
    "                  c='black', marker='D', s=300, edgecolors='yellow', linewidths=2, zorder=10, label='Medias')\n",
    "silhouette_gmm = silhouette_score(X_gmm, y_gmm_pred)\n",
    "axes[0, 2].set_title(f'GMM (Hard Clustering)\\nSilhouette: {silhouette_gmm:.3f}', fontweight='bold', fontsize=12)\n",
    "axes[0, 2].set_xlabel('Feature 1')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. GMM - Probabilidades de Pertenencia (Cluster 0)\n",
    "scatter = axes[1, 0].scatter(X_gmm[:, 0], X_gmm[:, 1], c=y_gmm_proba[:, 0], \n",
    "                             cmap='RdYlGn', s=60, alpha=0.8, edgecolors='k', vmin=0, vmax=1)\n",
    "plt.colorbar(scatter, ax=axes[1, 0], label='P(Cluster 0)')\n",
    "axes[1, 0].set_title('GMM: Probabilidad Cluster 0', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Feature 1')\n",
    "axes[1, 0].set_ylabel('Feature 2')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. GMM - Muestras Generadas (Modelo Generativo)\n",
    "X_generated, y_generated = gmm.sample(300)\n",
    "axes[1, 1].scatter(X_generated[:, 0], X_generated[:, 1], c=y_generated, \n",
    "                  cmap='Set1', s=50, alpha=0.6, edgecolors='k', marker='s')\n",
    "axes[1, 1].scatter(gmm.means_[:, 0], gmm.means_[:, 1],\n",
    "                  c='black', marker='D', s=300, edgecolors='yellow', linewidths=2, zorder=10)\n",
    "axes[1, 1].set_title('Datos Generados por GMM\\n(Modelo Generativo)', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_xlabel('Feature 1')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Elipses de covarianza GMM\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "axes[1, 2].scatter(X_gmm[:, 0], X_gmm[:, 1], c=y_gmm_pred, cmap='Set1', s=30, alpha=0.5, edgecolors='k')\n",
    "axes[1, 2].scatter(gmm.means_[:, 0], gmm.means_[:, 1],\n",
    "                  c='black', marker='D', s=300, edgecolors='yellow', linewidths=2, zorder=10)\n",
    "\n",
    "# Dibujar elipses de covarianza (2 desviaciones est√°ndar)\n",
    "for i in range(3):\n",
    "    covariance = gmm.covariances_[i]\n",
    "    v, w = np.linalg.eigh(covariance)\n",
    "    v = 2.0 * np.sqrt(2.0) * np.sqrt(v)  # 2 std deviations\n",
    "    u = w[0] / np.linalg.norm(w[0])\n",
    "    angle = np.arctan2(u[1], u[0])\n",
    "    angle = 180.0 * angle / np.pi\n",
    "    \n",
    "    ell = Ellipse(gmm.means_[i], v[0], v[1], angle=180.0 + angle, \n",
    "                  edgecolor=f'C{i}', facecolor='none', linewidth=3, linestyle='--')\n",
    "    axes[1, 2].add_patch(ell)\n",
    "\n",
    "axes[1, 2].set_title('GMM: Elipses de Covarianza\\n(2œÉ)', fontweight='bold', fontsize=12)\n",
    "axes[1, 2].set_xlabel('Feature 1')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä An√°lisis Detallado:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"GMM - BIC Score: {gmm.bic(X_gmm):.2f} (menor es mejor)\")\n",
    "print(f\"GMM - AIC Score: {gmm.aic(X_gmm):.2f} (menor es mejor)\")\n",
    "print(f\"GMM - Log-Likelihood: {gmm.score(X_gmm):.2f}\")\n",
    "\n",
    "print(\"\\nüìà Pesos de las Componentes:\")\n",
    "for i, weight in enumerate(gmm.weights_):\n",
    "    print(f\"   Cluster {i}: {weight:.3f} ({weight*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nüí° Ventajas de GMM sobre K-Means:\")\n",
    "print(\"   ‚úÖ Soft clustering: probabilidades en lugar de asignaciones binarias\")\n",
    "print(\"   ‚úÖ Modelo generativo: puede generar nuevas muestras\")\n",
    "print(\"   ‚úÖ Captura forma el√≠ptica mediante covarianzas completas\")\n",
    "print(\"   ‚úÖ Fundamento probabil√≠stico riguroso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fa2117",
   "metadata": {},
   "source": [
    "## 6. Reducci√≥n de Dimensionalidad: PCA\n",
    "\n",
    "### üìâ Principal Component Analysis\n",
    "\n",
    "PCA identifica **direcciones de m√°xima varianza** en los datos, proyectando observaciones a un subespacio de menor dimensi√≥n.\n",
    "\n",
    "#### Fundamento Matem√°tico\n",
    "\n",
    "Dada matriz de datos centrados $X \\in \\mathbb{R}^{n \\times d}$, PCA busca proyecci√≥n ortogonal $W \\in \\mathbb{R}^{d \\times k}$ que maximiza varianza:\n",
    "\n",
    "$$\\max_W \\text{tr}(W^T\\Sigma W) \\quad \\text{sujeto a } W^TW = I_k$$\n",
    "\n",
    "donde $\\Sigma = \\frac{1}{n}X^TX$ es la matriz de covarianza muestral.\n",
    "\n",
    "#### Soluci√≥n:\n",
    "Los **vectores propios** de $\\Sigma$ correspondientes a los $k$ mayores valores propios forman las columnas de $W$.\n",
    "\n",
    "Equivalentemente: $X = UDV^T$ (Singular Value Decomposition)\n",
    "\n",
    "### Propiedades Clave:\n",
    "- Transformaci√≥n lineal √≥ptima bajo criterio de reconstrucci√≥n de m√≠nimos cuadrados\n",
    "- Componentes principales son **ortogonales** (no correlacionados)\n",
    "- Varianza explicada por PC $k$: $\\lambda_k / \\sum_i \\lambda_i$\n",
    "\n",
    "### ‚úÖ Aplicaciones:\n",
    "- Compresi√≥n de datos\n",
    "- Visualizaci√≥n mediante proyecci√≥n a 2D/3D\n",
    "- Eliminaci√≥n de multicolinealidad\n",
    "- Pre-procesamiento para aceleraci√≥n de algoritmos\n",
    "- Reducci√≥n de ruido\n",
    "\n",
    "### ‚ö†Ô∏è Limitaciones:\n",
    "- Asume **linealidad** de relaciones\n",
    "- Sensible a escala (requiere normalizaci√≥n)\n",
    "- Componentes pueden ser dif√≠ciles de interpretar\n",
    "- No preserva distancias locales necesariamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed9c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Pr√°ctico: PCA - Visualizaci√≥n y Reducci√≥n de Dimensionalidad\n",
    "\n",
    "print(\"üìâ Aplicando PCA a Digits Dataset (64 dimensiones ‚Üí 2D/3D)...\\n\")\n",
    "\n",
    "# Cargar datos de d√≠gitos manuscritos (8x8 p√≠xeles = 64 caracter√≠sticas)\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "print(f\"Dataset original: {X_digits.shape[0]} muestras, {X_digits.shape[1]} caracter√≠sticas\")\n",
    "print(f\"Clases: {len(np.unique(y_digits))} d√≠gitos (0-9)\")\n",
    "\n",
    "# Normalizar datos\n",
    "scaler_pca = StandardScaler()\n",
    "X_digits_scaled = scaler_pca.fit_transform(X_digits)\n",
    "\n",
    "# Aplicar PCA con todos los componentes para an√°lisis de varianza\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_digits_scaled)\n",
    "\n",
    "# Crear visualizaciones\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# 1. Varianza explicada por componente\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "ax1.bar(range(1, 21), pca_full.explained_variance_ratio_[:20] * 100, alpha=0.7, color='steelblue')\n",
    "ax1.set_xlabel('Componente Principal')\n",
    "ax1.set_ylabel('Varianza Explicada (%)')\n",
    "ax1.set_title('Varianza Explicada por PC\\n(Top 20 componentes)', fontweight='bold', fontsize=11)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Varianza acumulada\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "cumsum_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "ax2.plot(range(1, len(cumsum_variance) + 1), cumsum_variance * 100, 'o-', linewidth=2, markersize=4)\n",
    "ax2.axhline(y=95, color='r', linestyle='--', linewidth=2, label='95% varianza')\n",
    "ax2.axhline(y=90, color='orange', linestyle='--', linewidth=2, label='90% varianza')\n",
    "ax2.set_xlabel('N√∫mero de Componentes')\n",
    "ax2.set_ylabel('Varianza Acumulada (%)')\n",
    "ax2.set_title('Varianza Explicada Acumulada', fontweight='bold', fontsize=11)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim([0, 50])\n",
    "\n",
    "# Encontrar n√∫mero de componentes para 95% varianza\n",
    "n_components_95 = np.argmax(cumsum_variance >= 0.95) + 1\n",
    "print(f\"\\nüìä Componentes necesarios para 95% varianza: {n_components_95}/{X_digits.shape[1]}\")\n",
    "\n",
    "# 3. Primeros 6 componentes principales visualizados\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "for i in range(6):\n",
    "    ax_small = plt.subplot(2, 6, i + 7)\n",
    "    component_image = pca_full.components_[i].reshape(8, 8)\n",
    "    ax_small.imshow(component_image, cmap='RdBu_r', aspect='auto')\n",
    "    ax_small.set_title(f'PC{i+1}\\n{pca_full.explained_variance_ratio_[i]*100:.1f}%', fontsize=9)\n",
    "    ax_small.axis('off')\n",
    "\n",
    "# 4. PCA 2D - Visualizaci√≥n\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_digits_scaled)\n",
    "\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "scatter = ax4.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y_digits, cmap='tab10', \n",
    "                     s=20, alpha=0.6, edgecolors='none')\n",
    "ax4.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% var)')\n",
    "ax4.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% var)')\n",
    "ax4.set_title(f'PCA 2D Projection\\nVarianza total: {sum(pca_2d.explained_variance_ratio_)*100:.1f}%', \n",
    "             fontweight='bold', fontsize=11)\n",
    "plt.colorbar(scatter, ax=ax4, label='D√≠gito', ticks=range(10))\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Reconstrucci√≥n con diferentes n√∫meros de componentes\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "\n",
    "# Seleccionar una muestra (d√≠gito '5')\n",
    "sample_idx = np.where(y_digits == 5)[0][0]\n",
    "original_digit = X_digits[sample_idx].reshape(8, 8)\n",
    "\n",
    "# Reconstruir con diferentes n√∫meros de componentes\n",
    "n_components_list = [2, 5, 10, 20, 64]\n",
    "reconstructions = []\n",
    "\n",
    "for n_comp in n_components_list:\n",
    "    if n_comp <= 64:\n",
    "        pca_temp = PCA(n_components=n_comp)\n",
    "        pca_temp.fit(X_digits_scaled)\n",
    "        transformed = pca_temp.transform(X_digits_scaled[sample_idx:sample_idx+1])\n",
    "        reconstructed_scaled = pca_temp.inverse_transform(transformed)\n",
    "        reconstructed = scaler_pca.inverse_transform(reconstructed_scaled)\n",
    "        reconstructions.append(reconstructed.reshape(8, 8))\n",
    "    else:\n",
    "        reconstructions.append(original_digit)\n",
    "\n",
    "# Mostrar reconstrucciones\n",
    "for idx, (recon, n_comp) in enumerate(zip(reconstructions, n_components_list)):\n",
    "    ax_recon = plt.subplot(2, 6, idx + 1)\n",
    "    ax_recon.imshow(recon, cmap='gray', aspect='auto')\n",
    "    \n",
    "    # Calcular MSE\n",
    "    mse = np.mean((original_digit - recon) ** 2)\n",
    "    ax_recon.set_title(f'{n_comp} PCs\\nMSE: {mse:.2f}', fontsize=9)\n",
    "    ax_recon.axis('off')\n",
    "\n",
    "# 6. PCA aplicado a clustering\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "\n",
    "# Aplicar K-Means en espacio PCA reducido\n",
    "kmeans_pca = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "y_kmeans_pca = kmeans_pca.fit_predict(X_pca_2d)\n",
    "\n",
    "scatter2 = ax6.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y_kmeans_pca, cmap='tab10', \n",
    "                      s=20, alpha=0.6, edgecolors='none')\n",
    "ax6.scatter(kmeans_pca.cluster_centers_[:, 0], kmeans_pca.cluster_centers_[:, 1],\n",
    "           c='red', marker='X', s=200, edgecolors='black', linewidths=2, zorder=10)\n",
    "ax6.set_xlabel('PC1')\n",
    "ax6.set_ylabel('PC2')\n",
    "ax6.set_title('K-Means en Espacio PCA\\n(10 clusters)', fontweight='bold', fontsize=11)\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Resumen PCA:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   ‚Ä¢ Varianza explicada por PC1: {pca_2d.explained_variance_ratio_[0]*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Varianza explicada por PC2: {pca_2d.explained_variance_ratio_[1]*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Varianza total (2 PCs): {sum(pca_2d.explained_variance_ratio_)*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Componentes para 95% varianza: {n_components_95}/64\")\n",
    "print(f\"   ‚Ä¢ Reducci√≥n dimensional: 64D ‚Üí 2D ({(1-2/64)*100:.1f}% reducci√≥n)\")\n",
    "\n",
    "print(\"\\nüí° Aplicaciones Pr√°cticas:\")\n",
    "print(\"   ‚úÖ Visualizaci√≥n de datos de alta dimensi√≥n\")\n",
    "print(\"   ‚úÖ Pre-procesamiento para acelerar algoritmos (clustering, clasificaci√≥n)\")\n",
    "print(\"   ‚úÖ Compresi√≥n de datos (trade-off entre compresi√≥n y p√©rdida)\")\n",
    "print(\"   ‚úÖ Eliminaci√≥n de ruido (reconstrucci√≥n con componentes principales)\")\n",
    "print(\"   ‚úÖ Detecci√≥n de multicolinealidad en features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd0a5b",
   "metadata": {},
   "source": [
    "## 7. t-SNE: Visualizaci√≥n No Lineal\n",
    "\n",
    "### üé® t-Distributed Stochastic Neighbor Embedding\n",
    "\n",
    "t-SNE es una t√©cnica **no lineal** de reducci√≥n de dimensionalidad especialmente efectiva para **visualizaci√≥n**, preservando estructura local de datos.\n",
    "\n",
    "#### Fundamento Matem√°tico\n",
    "\n",
    "**En espacio original**, convierte distancias en probabilidades condicionales:\n",
    "\n",
    "$$p_{j|i} = \\frac{\\exp(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i}\\exp(-\\|x_i - x_k\\|^2 / 2\\sigma_i^2)}$$\n",
    "\n",
    "**En espacio reducido**, usa distribuci√≥n t de Student (cola pesada):\n",
    "\n",
    "$$q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l}(1 + \\|y_k - y_l\\|^2)^{-1}}$$\n",
    "\n",
    "**Optimizaci√≥n**: Minimiza divergencia KL entre $P$ y $Q$:\n",
    "\n",
    "$$\\text{KL}(P\\|Q) = \\sum_{i \\neq j} p_{ij}\\log\\frac{p_{ij}}{q_{ij}}$$\n",
    "\n",
    "### Par√°metros Clave:\n",
    "\n",
    "- **perplexity**: Controla n√∫mero efectivo de vecinos cercanos (t√≠pico: 5-50)\n",
    "  - Bajo: √©nfasis en estructura local\n",
    "  - Alto: √©nfasis en estructura global\n",
    "\n",
    "- **learning_rate**: Tasa de aprendizaje (t√≠pico: 10-1000)\n",
    "\n",
    "- **n_iter**: Iteraciones de optimizaci√≥n (m√≠nimo: 250, recomendado: 1000+)\n",
    "\n",
    "### ‚úÖ Ventajas:\n",
    "- Preserva estructura local (vecindarios)\n",
    "- Visualizaci√≥n efectiva de datos complejos\n",
    "- Revela clusters no lineales\n",
    "\n",
    "### ‚ö†Ô∏è Limitaciones:\n",
    "- **No determin√≠stico** (diferentes ejecuciones dan resultados diferentes)\n",
    "- **Computacionalmente costoso**: $O(n^2)$ (mitigado con Barnes-Hut)\n",
    "- **Solo para visualizaci√≥n** (no para reducci√≥n dimensional en pipeline)\n",
    "- Distancias globales no preservadas\n",
    "- Sensible a par√°metros (perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f6ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Pr√°ctico: PCA vs t-SNE - Comparaci√≥n en Datos Complejos\n",
    "\n",
    "print(\"üé® Comparando PCA (lineal) vs t-SNE (no lineal) en Digits Dataset...\\n\")\n",
    "\n",
    "# Usar submuestra para t-SNE (m√°s r√°pido)\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "indices = np.random.choice(len(X_digits), n_samples, replace=False)\n",
    "X_sample = X_digits_scaled[indices]\n",
    "y_sample = y_digits[indices]\n",
    "\n",
    "print(f\"Usando {n_samples} muestras para comparaci√≥n...\")\n",
    "\n",
    "# Aplicar PCA (ya calculado anteriormente, recalcular para subset)\n",
    "pca_comp = PCA(n_components=2)\n",
    "X_pca_comp = pca_comp.fit_transform(X_sample)\n",
    "\n",
    "# Aplicar t-SNE con diferentes perplexities\n",
    "perplexities = [5, 30, 50]\n",
    "tsne_results = []\n",
    "\n",
    "print(\"\\nAplicando t-SNE con diferentes perplexities...\")\n",
    "for perp in perplexities:\n",
    "    print(f\"   ‚Ä¢ Perplexity {perp}...\", end='')\n",
    "    tsne = TSNE(n_components=2, perplexity=perp, random_state=42, n_iter=1000, \n",
    "                learning_rate='auto', init='pca')\n",
    "    X_tsne = tsne.fit_transform(X_sample)\n",
    "    tsne_results.append((perp, X_tsne))\n",
    "    print(\" ‚úì\")\n",
    "\n",
    "# Visualizaci√≥n comparativa\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# 1. PCA\n",
    "scatter1 = axes[0, 0].scatter(X_pca_comp[:, 0], X_pca_comp[:, 1], c=y_sample, \n",
    "                             cmap='tab10', s=30, alpha=0.7, edgecolors='k', linewidths=0.5)\n",
    "axes[0, 0].set_title(f'PCA (Lineal)\\nVarianza: {sum(pca_comp.explained_variance_ratio_)*100:.1f}%', \n",
    "                    fontweight='bold', fontsize=13)\n",
    "axes[0, 0].set_xlabel('PC1')\n",
    "axes[0, 0].set_ylabel('PC2')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0, 0], label='D√≠gito', ticks=range(10))\n",
    "\n",
    "# 2-4. t-SNE con diferentes perplexities\n",
    "for idx, (perp, X_tsne) in enumerate(tsne_results):\n",
    "    row = (idx + 1) // 2\n",
    "    col = (idx + 1) % 2\n",
    "    \n",
    "    scatter = axes[row, col].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_sample, \n",
    "                                    cmap='tab10', s=30, alpha=0.7, edgecolors='k', linewidths=0.5)\n",
    "    axes[row, col].set_title(f't-SNE (No Lineal)\\nPerplexity: {perp}', \n",
    "                            fontweight='bold', fontsize=13)\n",
    "    axes[row, col].set_xlabel('t-SNE Dim 1')\n",
    "    axes[row, col].set_ylabel('t-SNE Dim 2')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=axes[row, col], label='D√≠gito', ticks=range(10))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis cuantitativo: clustering en espacio reducido\n",
    "print(\"\\nüìä Evaluaci√≥n de Clustering en Espacio Reducido:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'M√©todo':<20} {'Silhouette':<15} {'Davies-Bouldin':<20} {'Calinski-Harabasz'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# PCA + K-Means\n",
    "kmeans_pca_eval = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "y_pred_pca = kmeans_pca_eval.fit_predict(X_pca_comp)\n",
    "sil_pca = silhouette_score(X_pca_comp, y_pred_pca)\n",
    "db_pca = davies_bouldin_score(X_pca_comp, y_pred_pca)\n",
    "ch_pca = calinski_harabasz_score(X_pca_comp, y_pred_pca)\n",
    "print(f\"{'PCA':<20} {sil_pca:<15.4f} {db_pca:<20.4f} {ch_pca:.2f}\")\n",
    "\n",
    "# t-SNE + K-Means (usando perplexity=30)\n",
    "X_tsne_30 = [x for p, x in tsne_results if p == 30][0]\n",
    "kmeans_tsne = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "y_pred_tsne = kmeans_tsne.fit_predict(X_tsne_30)\n",
    "sil_tsne = silhouette_score(X_tsne_30, y_pred_tsne)\n",
    "db_tsne = davies_bouldin_score(X_tsne_30, y_pred_tsne)\n",
    "ch_tsne = calinski_harabasz_score(X_tsne_30, y_pred_tsne)\n",
    "print(f\"{'t-SNE (perp=30)':<20} {sil_tsne:<15.4f} {db_tsne:<20.4f} {ch_tsne:.2f}\")\n",
    "\n",
    "print(\"\\nüí° Conclusiones:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ PCA:\")\n",
    "print(\"   ‚Ä¢ R√°pido y determin√≠stico\")\n",
    "print(\"   ‚Ä¢ Preserva varianza global\")\n",
    "print(\"   ‚Ä¢ √ötil para pre-procesamiento y compresi√≥n\")\n",
    "print(\"   ‚Ä¢ Limitado para estructuras no lineales\")\n",
    "\n",
    "print(\"\\n‚úÖ t-SNE:\")\n",
    "print(\"   ‚Ä¢ Visualizaci√≥n superior de estructuras complejas\")\n",
    "print(\"   ‚Ä¢ Preserva relaciones locales (vecindarios)\")\n",
    "print(\"   ‚Ä¢ Revela clusters no lineales claramente\")\n",
    "print(\"   ‚Ä¢ M√°s lento, no determin√≠stico\")\n",
    "print(\"   ‚Ä¢ Sensible a perplexity (experimentar con valores 5-50)\")\n",
    "\n",
    "print(\"\\nüéØ Recomendaciones:\")\n",
    "print(\"   ‚Ä¢ Usar PCA para an√°lisis exploratorio r√°pido y reducci√≥n dimensional\")\n",
    "print(\"   ‚Ä¢ Usar t-SNE para visualizaci√≥n final de alta calidad\")\n",
    "print(\"   ‚Ä¢ Combinar: PCA primero (64D‚Üí50D), luego t-SNE (50D‚Üí2D) para acelerar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d8129b",
   "metadata": {},
   "source": [
    "## 8. Detecci√≥n de Anomal√≠as\n",
    "\n",
    "### üö® Identificaci√≥n de Outliers\n",
    "\n",
    "La detecci√≥n de anomal√≠as identifica observaciones que se desv√≠an significativamente de patrones normales.\n",
    "\n",
    "#### Isolation Forest\n",
    "\n",
    "**Principio**: Las anomal√≠as son \"f√°ciles de aislar\" (requieren menos particiones en √°rboles aleatorios).\n",
    "\n",
    "**Anomaly Score**: Basado en longitud promedio de camino:\n",
    "\n",
    "$$s(x, n) = 2^{-\\frac{E(h(x))}{c(n)}}$$\n",
    "\n",
    "donde $h(x)$ es profundidad de aislamiento y $c(n)$ es profundidad promedio esperada.\n",
    "\n",
    "- Score ‚âà 1: Anomal√≠a\n",
    "- Score ‚âà 0.5: Normal\n",
    "- Score < 0.5: Normal con alta confianza\n",
    "\n",
    "#### Local Outlier Factor (LOF)\n",
    "\n",
    "**Principio**: Compara densidad local de un punto con densidad de sus vecinos.\n",
    "\n",
    "$$\\text{LOF}_k(x) = \\frac{\\sum_{o \\in N_k(x)} \\frac{\\text{lrd}(o)}{\\text{lrd}(x)}}{|N_k(x)|}$$\n",
    "\n",
    "donde $\\text{lrd}$ es local reachability density.\n",
    "\n",
    "- LOF ‚âà 1: Normal (densidad similar a vecinos)\n",
    "- LOF >> 1: Anomal√≠a (densidad menor que vecinos)\n",
    "\n",
    "### ‚úÖ Ventajas:\n",
    "- **Isolation Forest**: Eficiente, escalable, maneja alta dimensionalidad\n",
    "- **LOF**: Detecta anomal√≠as locales, flexible con diferentes densidades\n",
    "\n",
    "### ‚ö†Ô∏è Consideraciones:\n",
    "- Requiere especificar tasa de contaminaci√≥n esperada\n",
    "- Sensible a par√°metros (n_neighbors en LOF)\n",
    "- Evaluaci√≥n dif√≠cil sin ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8cc6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Pr√°ctico: Detecci√≥n de Anomal√≠as - Isolation Forest vs LOF\n",
    "\n",
    "print(\"üö® Aplicando algoritmos de detecci√≥n de anomal√≠as...\\n\")\n",
    "\n",
    "# Generar dataset con outliers\n",
    "np.random.seed(42)\n",
    "n_samples = 300\n",
    "n_outliers = 30\n",
    "\n",
    "# Datos normales\n",
    "X_normal = np.random.randn(n_samples, 2) * 0.5 + np.array([0, 0])\n",
    "\n",
    "# Outliers\n",
    "X_outliers = np.random.uniform(low=-4, high=4, size=(n_outliers, 2))\n",
    "\n",
    "# Combinar\n",
    "X_anomaly = np.vstack([X_normal, X_outliers])\n",
    "y_true = np.array([0] * n_samples + [1] * n_outliers)  # 0=normal, 1=outlier\n",
    "\n",
    "print(f\"Dataset: {len(X_anomaly)} puntos ({n_samples} normales + {n_outliers} outliers)\")\n",
    "\n",
    "# Aplicar Isolation Forest\n",
    "contamination_rate = n_outliers / len(X_anomaly)\n",
    "iso_forest = IsolationForest(contamination=contamination_rate, random_state=42, n_estimators=100)\n",
    "y_pred_iso = iso_forest.fit_predict(X_anomaly)\n",
    "y_pred_iso = np.where(y_pred_iso == -1, 1, 0)  # Convertir -1/1 a 1/0\n",
    "\n",
    "# Obtener anomaly scores\n",
    "anomaly_scores_iso = -iso_forest.score_samples(X_anomaly)  # Negar para que mayor = m√°s an√≥malo\n",
    "\n",
    "# Aplicar Local Outlier Factor\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=contamination_rate)\n",
    "y_pred_lof = lof.fit_predict(X_anomaly)\n",
    "y_pred_lof = np.where(y_pred_lof == -1, 1, 0)\n",
    "\n",
    "# Obtener LOF scores\n",
    "lof_scores = -lof.negative_outlier_factor_\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Ground Truth\n",
    "axes[0, 0].scatter(X_anomaly[y_true==0, 0], X_anomaly[y_true==0, 1], \n",
    "                  c='blue', label='Normal', s=50, alpha=0.6, edgecolors='k')\n",
    "axes[0, 0].scatter(X_anomaly[y_true==1, 0], X_anomaly[y_true==1, 1], \n",
    "                  c='red', label='Outlier', s=100, alpha=0.8, marker='X', edgecolors='black', linewidths=2)\n",
    "axes[0, 0].set_title('Ground Truth', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_xlabel('Feature 1')\n",
    "axes[0, 0].set_ylabel('Feature 2')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Isolation Forest - Predicciones\n",
    "axes[0, 1].scatter(X_anomaly[y_pred_iso==0, 0], X_anomaly[y_pred_iso==0, 1], \n",
    "                  c='blue', label='Normal', s=50, alpha=0.6, edgecolors='k')\n",
    "axes[0, 1].scatter(X_anomaly[y_pred_iso==1, 0], X_anomaly[y_pred_iso==1, 1], \n",
    "                  c='red', label='Outlier', s=100, alpha=0.8, marker='X', edgecolors='black', linewidths=2)\n",
    "\n",
    "# Calcular m√©tricas\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "precision_iso = precision_score(y_true, y_pred_iso)\n",
    "recall_iso = recall_score(y_true, y_pred_iso)\n",
    "f1_iso = f1_score(y_true, y_pred_iso)\n",
    "\n",
    "axes[0, 1].set_title(f'Isolation Forest\\nPrec: {precision_iso:.3f} | Rec: {recall_iso:.3f} | F1: {f1_iso:.3f}', \n",
    "                    fontweight='bold', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Feature 1')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. LOF - Predicciones\n",
    "axes[0, 2].scatter(X_anomaly[y_pred_lof==0, 0], X_anomaly[y_pred_lof==0, 1], \n",
    "                  c='blue', label='Normal', s=50, alpha=0.6, edgecolors='k')\n",
    "axes[0, 2].scatter(X_anomaly[y_pred_lof==1, 0], X_anomaly[y_pred_lof==1, 1], \n",
    "                  c='red', label='Outlier', s=100, alpha=0.8, marker='X', edgecolors='black', linewidths=2)\n",
    "\n",
    "precision_lof = precision_score(y_true, y_pred_lof)\n",
    "recall_lof = recall_score(y_true, y_pred_lof)\n",
    "f1_lof = f1_score(y_true, y_pred_lof)\n",
    "\n",
    "axes[0, 2].set_title(f'Local Outlier Factor\\nPrec: {precision_lof:.3f} | Rec: {recall_lof:.3f} | F1: {f1_lof:.3f}', \n",
    "                    fontweight='bold', fontsize=12)\n",
    "axes[0, 2].set_xlabel('Feature 1')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Isolation Forest - Anomaly Scores\n",
    "scatter_iso_scores = axes[1, 0].scatter(X_anomaly[:, 0], X_anomaly[:, 1], \n",
    "                                       c=anomaly_scores_iso, cmap='RdYlGn_r', \n",
    "                                       s=60, alpha=0.7, edgecolors='k', linewidths=0.5)\n",
    "plt.colorbar(scatter_iso_scores, ax=axes[1, 0], label='Anomaly Score')\n",
    "axes[1, 0].set_title('Isolation Forest\\nAnomaly Scores', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Feature 1')\n",
    "axes[1, 0].set_ylabel('Feature 2')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. LOF - Scores\n",
    "scatter_lof_scores = axes[1, 1].scatter(X_anomaly[:, 0], X_anomaly[:, 1], \n",
    "                                       c=lof_scores, cmap='RdYlGn_r', \n",
    "                                       s=60, alpha=0.7, edgecolors='k', linewidths=0.5)\n",
    "plt.colorbar(scatter_lof_scores, ax=axes[1, 1], label='LOF Score')\n",
    "axes[1, 1].set_title('Local Outlier Factor\\nLOF Scores', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_xlabel('Feature 1')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Confusion Matrices\n",
    "cm_iso = confusion_matrix(y_true, y_pred_iso)\n",
    "cm_lof = confusion_matrix(y_true, y_pred_lof)\n",
    "\n",
    "ax_cm1 = plt.subplot(2, 6, 11)\n",
    "sns.heatmap(cm_iso, annot=True, fmt='d', cmap='Blues', ax=ax_cm1,\n",
    "           xticklabels=['Normal', 'Outlier'], yticklabels=['Normal', 'Outlier'])\n",
    "ax_cm1.set_title('Isolation Forest\\nConfusion Matrix', fontsize=10, fontweight='bold')\n",
    "ax_cm1.set_ylabel('True')\n",
    "ax_cm1.set_xlabel('Predicted')\n",
    "\n",
    "ax_cm2 = plt.subplot(2, 6, 12)\n",
    "sns.heatmap(cm_lof, annot=True, fmt='d', cmap='Greens', ax=ax_cm2,\n",
    "           xticklabels=['Normal', 'Outlier'], yticklabels=['Normal', 'Outlier'])\n",
    "ax_cm2.set_title('LOF\\nConfusion Matrix', fontsize=10, fontweight='bold')\n",
    "ax_cm2.set_ylabel('True')\n",
    "ax_cm2.set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis detallado\n",
    "print(\"\\nüìä Comparaci√≥n de Algoritmos:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Algoritmo':<20} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Isolation Forest':<20} {precision_iso:<12.4f} {recall_iso:<12.4f} {f1_iso:<12.4f}\")\n",
    "print(f\"{'LOF':<20} {precision_lof:<12.4f} {recall_lof:<12.4f} {f1_lof:<12.4f}\")\n",
    "\n",
    "print(\"\\nüí° Interpretaci√≥n:\")\n",
    "print(\"   ‚Ä¢ Precision: De los puntos marcados como outliers, ¬øcu√°ntos lo son realmente?\")\n",
    "print(\"   ‚Ä¢ Recall: De todos los outliers reales, ¬øcu√°ntos se detectaron?\")\n",
    "print(\"   ‚Ä¢ F1-Score: Media arm√≥nica de Precision y Recall\")\n",
    "\n",
    "print(\"\\nüéØ Recomendaciones de Uso:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ Isolation Forest:\")\n",
    "print(\"   ‚Ä¢ Escalable a grandes datasets\")\n",
    "print(\"   ‚Ä¢ Funciona bien en alta dimensionalidad\")\n",
    "print(\"   ‚Ä¢ M√°s r√°pido que LOF\")\n",
    "print(\"   ‚Ä¢ Ideal para outliers globales\")\n",
    "\n",
    "print(\"\\n‚úÖ Local Outlier Factor:\")\n",
    "print(\"   ‚Ä¢ Detecta anomal√≠as locales (outliers contextuales)\")\n",
    "print(\"   ‚Ä¢ Maneja clusters de diferentes densidades\")\n",
    "print(\"   ‚Ä¢ M√°s sensible a patrones locales\")\n",
    "print(\"   ‚Ä¢ Requiere ajuste de n_neighbors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68491b0",
   "metadata": {},
   "source": [
    "## 9. Caso Pr√°ctico: Wine Dataset\n",
    "\n",
    "Aplicaremos t√©cnicas de aprendizaje no supervisado al Wine Dataset para descubrir patrones sin usar las etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f443cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso Pr√°ctico Completo: Wine Dataset - Pipeline de An√°lisis No Supervisado\n",
    "\n",
    "print(\"üç∑ An√°lisis No Supervisado del Wine Dataset\\n\")\n",
    "\n",
    "# Cargar datos\n",
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine_true = wine.target  # Solo para evaluaci√≥n, no se usa en entrenamiento\n",
    "\n",
    "# Crear DataFrame\n",
    "df_wine = pd.DataFrame(X_wine, columns=wine.feature_names)\n",
    "df_wine['cultivar'] = y_wine_true\n",
    "\n",
    "print(f\"Dataset: {X_wine.shape[0]} muestras, {X_wine.shape[1]} caracter√≠sticas\")\n",
    "print(f\"Clases reales: {len(np.unique(y_wine_true))} cultivares de vino\\n\")\n",
    "\n",
    "print(\"üìä Primeras 5 filas:\")\n",
    "print(df_wine.head())\n",
    "\n",
    "# Normalizar datos\n",
    "scaler_wine = StandardScaler()\n",
    "X_wine_scaled = scaler_wine.fit_transform(X_wine)\n",
    "\n",
    "# Pipeline de an√°lisis no supervisado\n",
    "fig = plt.figure(figsize=(20, 14))\n",
    "\n",
    "# 1. Clustering: Comparaci√≥n de algoritmos\n",
    "print(\"\\nüî¨ 1. CLUSTERING - Comparaci√≥n de algoritmos...\")\n",
    "\n",
    "clustering_algos = {\n",
    "    'K-Means': KMeans(n_clusters=3, random_state=42, n_init=10),\n",
    "    'Hierarchical': AgglomerativeClustering(n_clusters=3, linkage='ward'),\n",
    "    'GMM': GaussianMixture(n_components=3, random_state=42, n_init=10)\n",
    "}\n",
    "\n",
    "clustering_results = {}\n",
    "\n",
    "for name, algo in clustering_algos.items():\n",
    "    if name == 'GMM':\n",
    "        algo.fit(X_wine_scaled)\n",
    "        y_pred = algo.predict(X_wine_scaled)\n",
    "    else:\n",
    "        y_pred = algo.fit_predict(X_wine_scaled)\n",
    "    \n",
    "    clustering_results[name] = {\n",
    "        'labels': y_pred,\n",
    "        'silhouette': silhouette_score(X_wine_scaled, y_pred),\n",
    "        'davies_bouldin': davies_bouldin_score(X_wine_scaled, y_pred),\n",
    "        'calinski': calinski_harabasz_score(X_wine_scaled, y_pred)\n",
    "    }\n",
    "\n",
    "# Tabla de resultados clustering\n",
    "print(\"\\nüìà M√©tricas de Clustering:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Algoritmo':<15} {'Silhouette':<15} {'Davies-Bouldin':<20} {'Calinski-Harabasz'}\")\n",
    "print(\"-\" * 80)\n",
    "for name, results in clustering_results.items():\n",
    "    print(f\"{name:<15} {results['silhouette']:<15.4f} {results['davies_bouldin']:<20.4f} {results['calinski']:.2f}\")\n",
    "\n",
    "# 2. PCA - Reducci√≥n de dimensionalidad\n",
    "print(\"\\nüìâ 2. REDUCCI√ìN DE DIMENSIONALIDAD (PCA)...\")\n",
    "\n",
    "pca_wine = PCA()\n",
    "pca_wine.fit(X_wine_scaled)\n",
    "\n",
    "# Encontrar componentes para 95% varianza\n",
    "cumsum_var_wine = np.cumsum(pca_wine.explained_variance_ratio_)\n",
    "n_comp_95_wine = np.argmax(cumsum_var_wine >= 0.95) + 1\n",
    "\n",
    "print(f\"   ‚Ä¢ Componentes para 95% varianza: {n_comp_95_wine}/{X_wine.shape[1]}\")\n",
    "\n",
    "# Aplicar PCA con 2 componentes para visualizaci√≥n\n",
    "pca_2d_wine = PCA(n_components=2)\n",
    "X_wine_pca = pca_2d_wine.fit_transform(X_wine_scaled)\n",
    "\n",
    "print(f\"   ‚Ä¢ Varianza explicada (2 PCs): {sum(pca_2d_wine.explained_variance_ratio_)*100:.2f}%\")\n",
    "\n",
    "# 3. Visualizaciones\n",
    "# Subplot 1: PCA - Ground Truth\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "scatter1 = ax1.scatter(X_wine_pca[:, 0], X_wine_pca[:, 1], c=y_wine_true, \n",
    "                      cmap='viridis', s=80, alpha=0.7, edgecolors='k')\n",
    "ax1.set_title('PCA - Ground Truth\\n(Cultivares Reales)', fontweight='bold', fontsize=11)\n",
    "ax1.set_xlabel(f'PC1 ({pca_2d_wine.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax1.set_ylabel(f'PC2 ({pca_2d_wine.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "plt.colorbar(scatter1, ax=ax1, label='Cultivar')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplots 2-4: Clustering en espacio PCA\n",
    "for idx, (name, results) in enumerate(clustering_results.items()):\n",
    "    ax = plt.subplot(3, 3, idx + 2)\n",
    "    scatter = ax.scatter(X_wine_pca[:, 0], X_wine_pca[:, 1], c=results['labels'], \n",
    "                        cmap='plasma', s=80, alpha=0.7, edgecolors='k')\n",
    "    ax.set_title(f'{name} en PCA\\nSil: {results[\"silhouette\"]:.3f}', \n",
    "                fontweight='bold', fontsize=11)\n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "    plt.colorbar(scatter, ax=ax, label='Cluster')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 5: Varianza explicada PCA\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "ax5.bar(range(1, len(pca_wine.explained_variance_ratio_) + 1), \n",
    "       pca_wine.explained_variance_ratio_ * 100, alpha=0.7, color='steelblue')\n",
    "ax5.axhline(y=pca_wine.explained_variance_ratio_[0]*100, color='r', \n",
    "           linestyle='--', alpha=0.7, label=f'PC1: {pca_wine.explained_variance_ratio_[0]*100:.1f}%')\n",
    "ax5.set_xlabel('Componente Principal')\n",
    "ax5.set_ylabel('Varianza Explicada (%)')\n",
    "ax5.set_title('Scree Plot - PCA', fontweight='bold', fontsize=11)\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Subplot 6: Feature importance en PC1 y PC2\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "features_to_show = 8  # Top 8 features\n",
    "pc1_importance = np.abs(pca_2d_wine.components_[0])\n",
    "pc2_importance = np.abs(pca_2d_wine.components_[1])\n",
    "\n",
    "top_features_pc1 = np.argsort(pc1_importance)[-features_to_show:]\n",
    "x_pos = np.arange(features_to_show)\n",
    "width = 0.35\n",
    "\n",
    "ax6.barh(x_pos, pc1_importance[top_features_pc1], width, label='PC1', alpha=0.8)\n",
    "ax6.barh(x_pos + width, pc2_importance[top_features_pc1], width, label='PC2', alpha=0.8)\n",
    "ax6.set_yticks(x_pos + width / 2)\n",
    "ax6.set_yticklabels([wine.feature_names[i][:20] for i in top_features_pc1], fontsize=8)\n",
    "ax6.set_xlabel('Importancia Absoluta')\n",
    "ax6.set_title('Top Features en PC1 y PC2', fontweight='bold', fontsize=11)\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Subplot 7: Dendrograma jer√°rquico\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "linkage_wine = linkage(X_wine_scaled[:50], method='ward')  # Subset para claridad\n",
    "dendrogram(linkage_wine, ax=ax7)\n",
    "ax7.set_title('Dendrograma Jer√°rquico\\n(Primeras 50 muestras)', fontweight='bold', fontsize=11)\n",
    "ax7.set_xlabel('√çndice de Muestra')\n",
    "ax7.set_ylabel('Distancia')\n",
    "ax7.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Subplot 8: Heatmap de correlaci√≥n (top features)\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "top_10_features = np.argsort(pc1_importance)[-10:]\n",
    "corr_matrix = np.corrcoef(X_wine[:, top_10_features].T)\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "           xticklabels=[wine.feature_names[i][:10] for i in top_10_features],\n",
    "           yticklabels=[wine.feature_names[i][:10] for i in top_10_features],\n",
    "           ax=ax8, cbar_kws={'label': 'Correlaci√≥n'})\n",
    "ax8.set_title('Correlaci√≥n entre Top 10 Features', fontweight='bold', fontsize=11)\n",
    "plt.setp(ax8.xaxis.get_majorticklabels(), rotation=45, ha='right', fontsize=8)\n",
    "plt.setp(ax8.yaxis.get_majorticklabels(), fontsize=8)\n",
    "\n",
    "# Subplot 9: Comparaci√≥n m√©tricas clustering\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "algos = list(clustering_results.keys())\n",
    "silhouette_vals = [clustering_results[a]['silhouette'] for a in algos]\n",
    "db_vals = [clustering_results[a]['davies_bouldin'] for a in algos]\n",
    "\n",
    "x_pos = np.arange(len(algos))\n",
    "width = 0.35\n",
    "\n",
    "ax9_twin = ax9.twinx()\n",
    "bars1 = ax9.bar(x_pos - width/2, silhouette_vals, width, label='Silhouette', alpha=0.8, color='green')\n",
    "bars2 = ax9_twin.bar(x_pos + width/2, db_vals, width, label='Davies-Bouldin', alpha=0.8, color='orange')\n",
    "\n",
    "ax9.set_ylabel('Silhouette Score', color='green')\n",
    "ax9.tick_params(axis='y', labelcolor='green')\n",
    "ax9_twin.set_ylabel('Davies-Bouldin Index', color='orange')\n",
    "ax9_twin.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "ax9.set_xticks(x_pos)\n",
    "ax9.set_xticklabels(algos)\n",
    "ax9.set_title('Comparaci√≥n de M√©tricas\\n(Mayor Sil mejor, Menor DB mejor)', fontweight='bold', fontsize=11)\n",
    "ax9.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# A√±adir leyendas\n",
    "lines1, labels1 = ax9.get_legend_handles_labels()\n",
    "lines2, labels2 = ax9_twin.get_legend_handles_labels()\n",
    "ax9.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Resumen final\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESUMEN DEL AN√ÅLISIS NO SUPERVISADO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_clustering = max(clustering_results.items(), key=lambda x: x[1]['silhouette'])\n",
    "print(f\"\\nüèÜ Mejor algoritmo de clustering: {best_clustering[0]}\")\n",
    "print(f\"   ‚Ä¢ Silhouette Score: {best_clustering[1]['silhouette']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Davies-Bouldin Index: {best_clustering[1]['davies_bouldin']:.4f}\")\n",
    "\n",
    "print(f\"\\nüìâ Reducci√≥n de dimensionalidad (PCA):\")\n",
    "print(f\"   ‚Ä¢ {n_comp_95_wine} componentes capturan 95% de la varianza\")\n",
    "print(f\"   ‚Ä¢ PC1 explica {pca_wine.explained_variance_ratio_[0]*100:.2f}% de la varianza\")\n",
    "print(f\"   ‚Ä¢ Top feature en PC1: {wine.feature_names[np.argmax(np.abs(pca_2d_wine.components_[0]))]}\")\n",
    "\n",
    "print(\"\\nüí° Conclusiones:\")\n",
    "print(\"   ‚úÖ Los algoritmos de clustering identifican estructuras consistentes con cultivares\")\n",
    "print(\"   ‚úÖ PCA reduce efectivamente la dimensionalidad preservando informaci√≥n\")\n",
    "print(\"   ‚úÖ Las caracter√≠sticas qu√≠micas permiten discriminar entre vinos\")\n",
    "print(\"   ‚úÖ Validaci√≥n con ground truth confirma calidad de los clusters descubiertos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0376696d",
   "metadata": {},
   "source": [
    "## 10. Conclusiones y Mejores Pr√°cticas\n",
    "\n",
    "### üìö Resumen de Conceptos Clave\n",
    "\n",
    "1. **Clustering**:\n",
    "   - **K-Means**: R√°pido, eficiente, pero limitado a clusters esf√©ricos\n",
    "   - **Jer√°rquico**: Visualizaci√≥n con dendrogramas, sin necesidad de especificar K\n",
    "   - **DBSCAN**: Detecta formas arbitrarias y outliers\n",
    "   - **GMM**: Soft clustering probabil√≠stico con fundamento estad√≠stico\n",
    "\n",
    "2. **Reducci√≥n de Dimensionalidad**:\n",
    "   - **PCA**: Transformaci√≥n lineal que maximiza varianza\n",
    "   - **t-SNE**: Visualizaci√≥n no lineal preservando estructura local\n",
    "\n",
    "3. **Detecci√≥n de Anomal√≠as**:\n",
    "   - **Isolation Forest**: Escalable, eficiente para outliers globales\n",
    "   - **LOF**: Detecta anomal√≠as locales considerando densidad\n",
    "\n",
    "### ‚úÖ Mejores Pr√°cticas\n",
    "\n",
    "#### Pre-procesamiento:\n",
    "- **Normalizaci√≥n**: Esencial para algoritmos basados en distancia (K-Means, DBSCAN, PCA)\n",
    "- **An√°lisis Exploratorio**: Visualizar distribuciones antes de aplicar algoritmos\n",
    "- **Manejo de Outliers**: Considerar detecci√≥n y tratamiento antes de clustering\n",
    "\n",
    "#### Selecci√≥n de Algoritmos:\n",
    "- **Clustering**:\n",
    "  - K-Means: Clusters esf√©ricos, datasets grandes\n",
    "  - DBSCAN: Formas irregulares, presencia de ruido\n",
    "  - Jer√°rquico: Visualizaci√≥n de jerarqu√≠as, datasets peque√±os/medianos\n",
    "  - GMM: Necesidad de probabilidades, clusters el√≠pticos\n",
    "  \n",
    "- **Reducci√≥n Dimensional**:\n",
    "  - PCA: Pre-procesamiento, compresi√≥n, an√°lisis exploratorio r√°pido\n",
    "  - t-SNE: Visualizaci√≥n final de alta calidad\n",
    "\n",
    "#### Evaluaci√≥n:\n",
    "- **M√©tricas Internas**: Silhouette, Davies-Bouldin, Calinski-Harabasz\n",
    "- **Validaci√≥n con Dominio**: Interpretar clusters con conocimiento experto\n",
    "- **Estabilidad**: Probar con diferentes inicializaciones y par√°metros\n",
    "\n",
    "#### Selecci√≥n de Hiperpar√°metros:\n",
    "- **K-Means**: Elbow method, Silhouette analysis para K √≥ptimo\n",
    "- **DBSCAN**: An√°lisis de k-distance plot para Œµ\n",
    "- **PCA**: Scree plot, varianza acumulada (t√≠picamente 90-95%)\n",
    "- **t-SNE**: Experimentar con perplexity (5-50)\n",
    "\n",
    "### üéØ Ejercicios Propuestos\n",
    "\n",
    "1. **Ejercicio 1**: Aplica clustering al Iris dataset y compara resultados con etiquetas reales\n",
    "2. **Ejercicio 2**: Usa PCA para comprimir im√°genes (MNIST) y eval√∫a reconstrucci√≥n vs componentes\n",
    "3. **Ejercicio 3**: Implementa detecci√≥n de fraude con Isolation Forest en dataset sint√©tico\n",
    "4. **Ejercicio 4**: Compara t-SNE con diferentes perplexities en dataset de alta dimensi√≥n\n",
    "5. **Ejercicio 5**: Segmentaci√≥n de clientes usando K-Means + PCA en datos de e-commerce\n",
    "\n",
    "### üöÄ Aplicaciones Reales\n",
    "\n",
    "- **Segmentaci√≥n de Clientes**: Identificar grupos de comportamiento similar\n",
    "- **Compresi√≥n de Datos**: Reducir almacenamiento preservando informaci√≥n\n",
    "- **Detecci√≥n de Fraude**: Identificar transacciones an√≥malas\n",
    "- **An√°lisis de Im√°genes**: Segmentaci√≥n, compresi√≥n, feature extraction\n",
    "- **Bioinform√°tica**: Clustering de genes, reducci√≥n de dimensionalidad en gen√≥mica\n",
    "- **Sistemas de Recomendaci√≥n**: Agrupamiento de usuarios/productos similares\n",
    "\n",
    "### üìñ Recursos Adicionales\n",
    "\n",
    "- **Scikit-learn User Guide**: https://scikit-learn.org/stable/unsupervised_learning.html\n",
    "- **Libro**: \"Pattern Recognition and Machine Learning\" by Christopher Bishop\n",
    "- **Paper t-SNE**: Van der Maaten & Hinton (2008) - \"Visualizing Data using t-SNE\"\n",
    "- **DBSCAN Original**: Ester et al. (1996) - \"A Density-Based Algorithm\"\n",
    "- **Kaggle**: Datasets y competiciones de clustering y anomaly detection\n",
    "\n",
    "### ‚ö†Ô∏è Cuidados y Limitaciones\n",
    "\n",
    "- **Interpretabilidad**: Clusters no siempre tienen significado intr√≠nseco\n",
    "- **Escalabilidad**: Algunos algoritmos (jer√°rquico, t-SNE) no escalan bien\n",
    "- **Sensibilidad**: Resultados pueden variar con normalizaci√≥n, inicializaci√≥n\n",
    "- **Evaluaci√≥n**: Dif√≠cil sin ground truth, requiere m√©tricas m√∫ltiples\n",
    "- **Curse of Dimensionality**: Distancias pierden significado en alta dimensionalidad\n",
    "\n",
    "---\n",
    "\n",
    "**üéì Siguiente Notebook**: [Validaci√≥n y Evaluaci√≥n](../03_Validacion_evaluacion/validacion_evaluacion.ipynb)\n",
    "\n",
    "En el siguiente m√≥dulo exploraremos t√©cnicas rigurosas de validaci√≥n y evaluaci√≥n de modelos de ML."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
