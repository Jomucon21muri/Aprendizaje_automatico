{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0325799",
   "metadata": {},
   "source": [
    "# üß† Deep Learning: Arquitecturas Neuronales Profundas\n",
    "## Representaciones Jer√°rquicas y Redes Neuronales\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/Jomucon21muri/Aprendizaje_automatico/blob/main/01_Sistemas_aprendizaje_automatico/04_Deep_learning/deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Resumen del Notebook\n",
    "\n",
    "Este notebook explora **Deep Learning**, el paradigma que ha revolucionado la IA mediante redes neuronales con m√∫ltiples capas que aprenden representaciones jer√°rquicas.\n",
    "\n",
    "### üéØ Objetivos de Aprendizaje\n",
    "\n",
    "1. **Fundamentos Te√≥ricos**:\n",
    "   - Aprendizaje de representaciones jer√°rquicas\n",
    "   - Retropropagaci√≥n y optimizaci√≥n en redes profundas\n",
    "   - Vanishing/Exploding gradients y soluciones\n",
    "\n",
    "2. **Redes Convolucionales (CNNs)**:\n",
    "   - Operaci√≥n de convoluci√≥n, pooling, stride, padding\n",
    "   - Arquitecturas: LeNet, AlexNet, VGG, ResNet\n",
    "   - Clasificaci√≥n de im√°genes (MNIST, CIFAR-10)\n",
    "   - Transfer Learning\n",
    "\n",
    "3. **Redes Recurrentes (RNNs/LSTMs)**:\n",
    "   - Procesamiento de secuencias temporales\n",
    "   - LSTM y GRU para memoria de largo plazo\n",
    "   - Aplicaciones en NLP y series temporales\n",
    "\n",
    "4. **T√©cnicas Avanzadas**:\n",
    "   - Dropout, Batch Normalization, Data Augmentation\n",
    "   - Autoencoders y reducci√≥n de dimensionalidad\n",
    "   - Fine-tuning de modelos pre-entrenados\n",
    "\n",
    "### üìä Contenido\n",
    "\n",
    "- Teor√≠a con fundamentaci√≥n matem√°tica\n",
    "- Implementaciones con TensorFlow/Keras\n",
    "- Visualizaciones de arquitecturas y feature maps\n",
    "- Ejemplos pr√°cticos completos\n",
    "- Comparaci√≥n de arquitecturas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece75834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n del entorno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Deep Learning con TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.datasets import mnist, cifar10, fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"‚úÖ Entorno configurado correctamente\")\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"Keras: {keras.__version__}\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPU disponible: {'S√≠ - ' + str(len(gpus)) + ' GPU(s)' if gpus else 'No (usando CPU)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9934fc8",
   "metadata": {},
   "source": [
    "## 1. Fundamentos: Red Neuronal Simple (MNIST)\n",
    "\n",
    "### üéØ Construcci√≥n de una Red Neuronal Densa\n",
    "\n",
    "Comenzaremos con una red fully-connected simple para clasificaci√≥n de d√≠gitos manuscritos (MNIST).\n",
    "\n",
    "**Arquitectura**:\n",
    "- Input: 784 p√≠xeles (28x28)\n",
    "- Hidden Layer 1: 128 neuronas + ReLU + Dropout(0.2)\n",
    "- Hidden Layer 2: 64 neuronas + ReLU + Dropout(0.2)\n",
    "- Output: 10 neuronas + Softmax (d√≠gitos 0-9)\n",
    "\n",
    "**Conceptos Clave**:\n",
    "- **ReLU**: $f(x) = \\max(0, x)$ - Evita vanishing gradient\n",
    "- **Softmax**: $\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$ - Probabilidades de clase\n",
    "- **Dropout**: Regularizaci√≥n que desactiva neuronas aleatoriamente\n",
    "- **Cross-Entropy Loss**: $\\mathcal{L} = -\\sum_{i} y_i \\log(\\hat{y}_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Completo: Red Neuronal Densa + CNN + Transfer Learning en MNIST y CIFAR-10\n",
    "\n",
    "print(\"üî¨ PARTE 1: RED NEURONAL DENSA EN MNIST\\n\")\n",
    "\n",
    "# Cargar MNIST\n",
    "(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = mnist.load_data()\n",
    "\n",
    "# Preprocesar\n",
    "X_train_mnist_flat = X_train_mnist.reshape(-1, 784).astype('float32') / 255.0\n",
    "X_test_mnist_flat = X_test_mnist.reshape(-1, 784).astype('float32') / 255.0\n",
    "y_train_mnist_cat = to_categorical(y_train_mnist, 10)\n",
    "y_test_mnist_cat = to_categorical(y_test_mnist, 10)\n",
    "\n",
    "print(f\"MNIST: {X_train_mnist.shape[0]} train, {X_test_mnist.shape[0]} test\")\n",
    "\n",
    "# Crear modelo denso\n",
    "model_dense = models.Sequential([\n",
    "    layers.Input(shape=(784,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_dense.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"\\nEntrenando red densa...\")\n",
    "history_dense = model_dense.fit(\n",
    "    X_train_mnist_flat, y_train_mnist_cat,\n",
    "    epochs=10, batch_size=128, verbose=0,\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "test_loss_dense, test_acc_dense = model_dense.evaluate(X_test_mnist_flat, y_test_mnist_cat, verbose=0)\n",
    "print(f\"‚úÖ Red Densa - Test Accuracy: {test_acc_dense:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ PARTE 2: CNN EN MNIST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Preprocesar para CNN (mantener estructura 2D)\n",
    "X_train_mnist_cnn = X_train_mnist.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "X_test_mnist_cnn = X_test_mnist.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "# Crear CNN (inspirada en LeNet)\n",
    "model_cnn = models.Sequential([\n",
    "    layers.Input(shape=(28, 28, 1)),\n",
    "    \n",
    "    # Bloque Conv 1\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Bloque Conv 2\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Bloque Conv 3\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    \n",
    "    # Clasificador\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"\\nEntrenando CNN...\")\n",
    "history_cnn = model_cnn.fit(\n",
    "    X_train_mnist_cnn, y_train_mnist_cat,\n",
    "    epochs=10, batch_size=128, verbose=0,\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "test_loss_cnn, test_acc_cnn = model_cnn.evaluate(X_test_mnist_cnn, y_test_mnist_cat, verbose=0)\n",
    "print(f\"‚úÖ CNN - Test Accuracy: {test_acc_cnn:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ PARTE 3: CNN EN CIFAR-10 (im√°genes a color)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cargar CIFAR-10 (subset para rapidez)\n",
    "(X_train_cifar, y_train_cifar), (X_test_cifar, y_test_cifar) = cifar10.load_data()\n",
    "\n",
    "# Usar subset\n",
    "X_train_cifar = X_train_cifar[:10000].astype('float32') / 255.0\n",
    "y_train_cifar = to_categorical(y_train_cifar[:10000], 10)\n",
    "X_test_cifar = X_test_cifar[:2000].astype('float32') / 255.0\n",
    "y_test_cifar = to_categorical(y_test_cifar[:2000], 10)\n",
    "\n",
    "print(f\"CIFAR-10: {X_train_cifar.shape[0]} train, {X_test_cifar.shape[0]} test\")\n",
    "\n",
    "# CNN para CIFAR-10\n",
    "model_cifar = models.Sequential([\n",
    "    layers.Input(shape=(32, 32, 3)),\n",
    "    \n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.4),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_cifar.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenando CNN en CIFAR-10...\")\n",
    "history_cifar = model_cifar.fit(\n",
    "    X_train_cifar, y_train_cifar,\n",
    "    epochs=20, batch_size=64, verbose=0,\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "test_loss_cifar, test_acc_cifar = model_cifar.evaluate(X_test_cifar, y_test_cifar, verbose=0)\n",
    "print(f\"‚úÖ CNN CIFAR-10 - Test Accuracy: {test_acc_cifar:.4f}\")\n",
    "\n",
    "# Visualizaci√≥n comprehensiva\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Subplot 1: Comparaci√≥n de arquitecturas en MNIST\n",
    "ax1 = plt.subplot(2, 4, 1)\n",
    "comparisons = ['Densa', 'CNN']\n",
    "accuracies = [test_acc_dense, test_acc_cnn]\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "bars = ax1.bar(comparisons, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('Test Accuracy', fontweight='bold')\n",
    "ax1.set_title('MNIST: Densa vs CNN', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylim([0.95, 1.0])\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Subplot 2: Training history - Red Densa\n",
    "ax2 = plt.subplot(2, 4, 2)\n",
    "ax2.plot(history_dense.history['accuracy'], 'b-', linewidth=2, label='Train', marker='o', markersize=4)\n",
    "ax2.plot(history_dense.history['val_accuracy'], 'r-', linewidth=2, label='Val', marker='s', markersize=4)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Red Densa - Learning Curves', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Training history - CNN MNIST\n",
    "ax3 = plt.subplot(2, 4, 3)\n",
    "ax3.plot(history_cnn.history['accuracy'], 'b-', linewidth=2, label='Train', marker='o', markersize=4)\n",
    "ax3.plot(history_cnn.history['val_accuracy'], 'r-', linewidth=2, label='Val', marker='s', markersize=4)\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.set_title('CNN MNIST - Learning Curves', fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 4: Training history - CNN CIFAR\n",
    "ax4 = plt.subplot(2, 4, 4)\n",
    "ax4.plot(history_cifar.history['accuracy'], 'b-', linewidth=2, label='Train', marker='o', markersize=3)\n",
    "ax4.plot(history_cifar.history['val_accuracy'], 'r-', linewidth=2, label='Val', marker='s', markersize=3)\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.set_title('CNN CIFAR-10 - Learning Curves', fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 5-6: Ejemplos de predicciones MNIST\n",
    "y_pred_mnist = model_cnn.predict(X_test_mnist_cnn[:10], verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred_mnist, axis=1)\n",
    "\n",
    "for i in range(6):\n",
    "    ax = plt.subplot(2, 8, i + 9)\n",
    "    ax.imshow(X_test_mnist[i], cmap='gray')\n",
    "    pred = y_pred_classes[i]\n",
    "    true = y_test_mnist[i]\n",
    "    color = 'green' if pred == true else 'red'\n",
    "    ax.set_title(f'P:{pred}\\nT:{true}', fontsize=9, color=color, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "# Subplot 7-8: Ejemplos de predicciones CIFAR-10\n",
    "y_pred_cifar_all = model_cifar.predict(X_test_cifar[:10], verbose=0)\n",
    "y_pred_cifar_classes = np.argmax(y_pred_cifar_all, axis=1)\n",
    "y_true_cifar_classes = np.argmax(y_test_cifar[:10], axis=1)\n",
    "cifar_names = ['airplane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "for i in range(6):\n",
    "    ax = plt.subplot(2, 8, i + 13)\n",
    "    ax.imshow(X_test_cifar[i])\n",
    "    pred = cifar_names[y_pred_cifar_classes[i]]\n",
    "    true = cifar_names[y_true_cifar_classes[i]]\n",
    "    color = 'green' if pred == true else 'red'\n",
    "    ax.set_title(f'{pred[:6]}\\n{true[:6]}', fontsize=8, color=color, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä RESUMEN DE RESULTADOS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"MNIST - Red Densa:     {test_acc_dense:.4f}\")\n",
    "print(f\"MNIST - CNN:           {test_acc_cnn:.4f}  (mejora: +{(test_acc_cnn-test_acc_dense)*100:.2f}%)\")\n",
    "print(f\"CIFAR-10 - CNN:        {test_acc_cifar:.4f}\")\n",
    "print(\"\\nüí° Observaciones:\")\n",
    "print(\"  ‚Ä¢ CNN supera a red densa en MNIST (estructura espacial)\")\n",
    "print(\"  ‚Ä¢ Batch Normalization y Dropout mejoran generalizaci√≥n\")\n",
    "print(\"  ‚Ä¢ CIFAR-10 es m√°s desafiante (im√°genes naturales a color)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffc2b57",
   "metadata": {},
   "source": [
    "## 2. Conclusiones y Mejores Pr√°cticas\n",
    "\n",
    "### üìö Conceptos Clave\n",
    "\n",
    "1. **Representaciones Jer√°rquicas**: Capas aprenden abstracciones progresivas\n",
    "2. **CNNs**: Convoluci√≥n + Pooling para procesar im√°genes eficientemente\n",
    "3. **RNNs/LSTMs**: Memoria para procesar secuencias temporales\n",
    "4. **Transfer Learning**: Reutilizar modelos pre-entrenados para nuevas tareas\n",
    "\n",
    "### ‚úÖ Mejores Pr√°cticas\n",
    "\n",
    "- **Normalizaci√≥n**: Escalar inputs a [0,1] o media=0, std=1\n",
    "- **Regularizaci√≥n**: Dropout, L1/L2, Early Stopping\n",
    "- **Batch Normalization**: Estabiliza entrenamiento\n",
    "- **Data Augmentation**: Aumentar datos para prevenir overfitting\n",
    "- **Learning Rate**: Usar learning rate decay o callbacks\n",
    "- **Arquitectura**: Empezar simple, incrementar complejidad gradualmente\n",
    "\n",
    "### üéØ Arquitecturas Principales\n",
    "\n",
    "| Arquitectura | A√±o | Innovaci√≥n Clave |\n",
    "|-------------|-----|------------------|\n",
    "| LeNet-5 | 1998 | Primera CNN exitosa |\n",
    "| AlexNet | 2012 | ReLU, Dropout, GPU training |\n",
    "| VGG | 2014 | Bloques repetitivos 3x3 conv |\n",
    "| ResNet | 2015 | Skip connections (residual) |\n",
    "| Inception | 2015 | Multi-scale feature extraction |\n",
    "| MobileNet | 2017 | Depthwise separable convolutions |\n",
    "\n",
    "### üöÄ Aplicaciones\n",
    "\n",
    "- **Visi√≥n Computacional**: Clasificaci√≥n, detecci√≥n, segmentaci√≥n\n",
    "- **NLP**: Traducci√≥n, sentiment analysis, chatbots\n",
    "- **Speech**: Reconocimiento de voz, s√≠ntesis\n",
    "- **Medicina**: Diagn√≥stico por imagen\n",
    "- **Autonomous Vehicles**: Detecci√≥n de objetos en tiempo real\n",
    "\n",
    "### üìñ Recursos\n",
    "\n",
    "- **Curso**: \"Deep Learning Specialization\" - Andrew Ng (Coursera)\n",
    "- **Libro**: \"Deep Learning\" - Goodfellow, Bengio, Courville\n",
    "- **Framework**: TensorFlow/Keras Documentation\n",
    "- **Papers**: https://paperswithcode.com/\n",
    "\n",
    "### üéØ Ejercicios\n",
    "\n",
    "1. Implementa ResNet con skip connections\n",
    "2. Entrena CNN con data augmentation en CIFAR-10\n",
    "3. Usa Transfer Learning con VGG16 pre-entrenado\n",
    "4. Implementa LSTM para predicci√≥n de series temporales\n",
    "5. Visualiza feature maps de capas convolucionales\n",
    "\n",
    "---\n",
    "\n",
    "**üéì Siguiente**: [Transformers](../05_Transformers/transformers.ipynb)\n",
    "\n",
    "En el siguiente m√≥dulo exploraremos la arquitectura Transformer, que revolucion√≥ NLP y ahora domina m√∫ltiples dominios de ML."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
