# Tarea: herramientas de procesamiento
## Big Data - Bloque 2

### Objetivo
Adquirir experiencia práctica con herramientas de procesamiento Big Data.

### Actividades

#### Actividad 1: instalación y configuración
- Instala Apache Spark (local o cloud)
- Configura entorno: Python, Jupyter
- Verifica instalación y crea primera aplicación

#### Actividad 2: PySpark Fundamentals
Implementa con PySpark:
- Cargar datos (CSV, JSON)
- Transformaciones básicas (map, filter, groupBy)
- Agregaciones (sum, count, avg)
- Joins entre DataFrames
- Ordenamiento y top-k

#### Actividad 3: Spark SQL
- Crea tablas temporales
- Ejecuta queries SQL
- Window functions
- Comparar rendimiento RDD vs DataFrame

#### Actividad 4: análisis de dataset real
Usando dataset público (Kaggle, etc.):
- Limpieza de datos
- Análisis exploratorio
- Agregaciones por dimensiones
- Resultados a parquet/csv

#### Actividad 5: comparación herramientas
- Implementa tarea simple con Spark y Dask
- Compara: velocidad, sintaxis, documentación
- Analiza cuándo usar cada una

### Criterios de evaluación
- Instalación y configuración: 15%
- Transformaciones PySpark: 25%
- SQL queries: 20%
- Análisis dataset real: 25%
- Comparación herramientas: 15%

### Entrega
- Notebook Jupyter con código comentado
- Dataset procesado (parquet/csv)
- Gráficos de resultados
- Documento comparación herramientas
