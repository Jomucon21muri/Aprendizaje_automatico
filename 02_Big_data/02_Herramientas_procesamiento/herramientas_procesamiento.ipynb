{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ca0a475",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Herramientas de Procesamiento Big Data\n",
    "## Spark, Hadoop, y Ecosistema Distribuido\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/Jomucon21muri/Aprendizaje_automatico/blob/main/02_Big_data/02_Herramientas_procesamiento/herramientas_procesamiento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Resumen\n",
    "\n",
    "Herramientas especializadas para procesamiento distribuido y paralelo de Big Data.\n",
    "\n",
    "### ‚ö° Apache Spark\n",
    "\n",
    "**Caracter√≠sticas**:\n",
    "- Procesamiento in-memory (100x m√°s r√°pido que MapReduce)\n",
    "- APIs: PySpark, Scala, Java, R\n",
    "- Componentes: Spark SQL, MLlib, GraphX, Streaming\n",
    "\n",
    "**Arquitectura**:\n",
    "- **Driver**: Coordina ejecuci√≥n\n",
    "- **Executors**: Procesan datos en paralelo\n",
    "- **Cluster Manager**: YARN, Mesos, Kubernetes\n",
    "\n",
    "### üêò Hadoop Ecosystem\n",
    "\n",
    "- **HDFS**: Almacenamiento distribuido\n",
    "- **MapReduce**: Procesamiento batch\n",
    "- **YARN**: Resource management\n",
    "- **Hive**: SQL sobre Hadoop\n",
    "- **HBase**: NoSQL database\n",
    "- **Pig**: Scripting para an√°lisis\n",
    "\n",
    "### üìä Comparaci√≥n\n",
    "\n",
    "| Aspecto | Hadoop MapReduce | Apache Spark |\n",
    "|---------|------------------|--------------|\n",
    "| Velocidad | Lento (disk I/O) | R√°pido (in-memory) |\n",
    "| Latencia | Alta | Baja |\n",
    "| Facilidad | Complejo | M√°s simple |\n",
    "| Streaming | Limitado | Excelente |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82048260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Simulaci√≥n de procesamiento distribuido\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import time\n",
    "\n",
    "def process_chunk(data_chunk):\n",
    "    \"\"\"Simular procesamiento de un chunk de datos\"\"\"\n",
    "    return data_chunk.sum()\n",
    "\n",
    "# Crear dataset grande\n",
    "np.random.seed(42)\n",
    "large_data = np.random.randn(1000000)\n",
    "\n",
    "# Dividir en chunks\n",
    "n_chunks = 4\n",
    "chunks = np.array_split(large_data, n_chunks)\n",
    "\n",
    "# Procesamiento paralelo\n",
    "start = time.time()\n",
    "with ProcessPoolExecutor(max_workers=n_chunks) as executor:\n",
    "    results = list(executor.map(process_chunk, chunks))\n",
    "parallel_time = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Procesamiento paralelo completado\")\n",
    "print(f\"   ‚Ä¢ Datos: {len(large_data):,} elementos\")\n",
    "print(f\"   ‚Ä¢ Chunks: {n_chunks}\")\n",
    "print(f\"   ‚Ä¢ Tiempo: {parallel_time:.4f}s\")\n",
    "print(f\"   ‚Ä¢ Resultado: {sum(results):.2f}\")\n",
    "print(\"\\nüîë Conceptos:\")\n",
    "print(\"  ‚Ä¢ Map: Aplicar funci√≥n a cada elemento\")\n",
    "print(\"  ‚Ä¢ Reduce: Agregar resultados\")\n",
    "print(\"  ‚Ä¢ Shuffle: Redistribuir datos\")\n",
    "print(\"  ‚Ä¢ Partitioning: Dividir datos eficientemente\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
